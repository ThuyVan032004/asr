{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "Mg_YAjQ9ttKS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "f96fca80-2b18-426d-8ec7-345b6bcbf351"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-dev.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pydub"
      ],
      "metadata": {
        "id": "euyV6PTthVWH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[2000:2050]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b17cc0-ca3b-428a-8509-8f4a1d58416b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-dev'\n",
        "\n",
        "train_size = int(0.8 * len(dev_df))\n",
        "train_df = dev_df.iloc[:train_size]\n",
        "test_df = dev_df.iloc[train_size+1:]\n",
        "\n",
        "train_audio_files = [os.path.join(f) for f in train_df['filename']]\n",
        "train_texts = train_df['text'].tolist()\n",
        "\n",
        "test_audio_files = [os.path.join(f) for f in test_df['filename']]\n",
        "test_texts = test_df['text'].tolist()"
      ],
      "metadata": {
        "id": "mzKLy1FmUBqo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<blank>\"],\n",
        "    vocab_size=6150,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in train_texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "IwEcJmCK_o1j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_paths, target_sr=16000, max_duration=1.5, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing pipeline for RNN-T\n",
        "    Returns:\n",
        "        log_mel: (batch_size, T, 80) tensor of log Mel spectrograms\n",
        "        lengths: (batch_size,) tensor of actual lengths in frames\n",
        "    \"\"\"\n",
        "    # Initialize mel transform once\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        win_length=400,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_specs = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    # Calculate max_len in frames based on max_duration\n",
        "    max_len = int(max_duration * target_sr / 160)  # hop_length is 160\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # 1. Load and resample\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "        waveform = waveform.to(device)\n",
        "\n",
        "        if sr != target_sr:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "        # 2. Normalize\n",
        "        waveform = torch.nn.functional.normalize(waveform, dim=-1)\n",
        "\n",
        "        # 3. Mel transform\n",
        "        mel_spec = mel_transform(waveform)  # (channels, n_mels, time)\n",
        "        log_mel = torch.log(mel_spec + 1e-6)\n",
        "        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n",
        "\n",
        "        # Permute to (channels, time, n_mels)\n",
        "        log_mel = log_mel.permute(0, 2, 1)\n",
        "        # print(\"Log mel: \", log_mel.shape)\n",
        "        # 4. Padding/trimming\n",
        "        if log_mel.size(1) > max_len:\n",
        "            log_mel = log_mel[:, :max_len+1, :]\n",
        "        else:\n",
        "            pad_amount = max_len + 1 - log_mel.size(1)\n",
        "            log_mel = torch.nn.functional.pad(log_mel, (0, 0, 0, pad_amount))\n",
        "\n",
        "        mel_specs.append(log_mel.squeeze(0))  # Remove channel dimension if single channel\n",
        "        audio_lengths.append(log_mel.size(1))\n",
        "\n",
        "    # Stack into batch (batch_size, T, n_mels)\n",
        "    mel_specs = torch.stack(mel_specs)\n",
        "    audio_lengths = torch.tensor(audio_lengths, dtype=torch.int32)\n",
        "\n",
        "    return mel_specs, audio_lengths"
      ],
      "metadata": {
        "id": "MTcU-suGxili"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.1 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, input_dim)\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)   # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # y: (B, U)\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        y = self.dropout(y)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        # Use a more robust approach with a multi-layer network\n",
        "        self.joint = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "\n",
        "        # Get batch size and sequence lengths\n",
        "        batch_size = h_enc.size(0)\n",
        "        T = h_enc.size(1)  # Audio sequence length\n",
        "        U = h_dec.size(1)  # Text sequence length\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "\n",
        "        # Expand to create the full cartesian product for alignment\n",
        "        h_enc = h_enc.expand(-1, -1, U, -1)  # (B, T, U, H)\n",
        "        h_dec = h_dec.expand(-1, T, -1, -1)  # (B, T, U, H)\n",
        "\n",
        "        # Concatenate features\n",
        "        joint = torch.cat([h_enc, h_dec], dim=-1)  # (B, T, U, 2H)\n",
        "\n",
        "        # Apply joint network to get logits\n",
        "        logits = self.joint(joint)  # (B, T, U, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(decoder_dim, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: (B, T, features), y: (B, U)\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "\n",
        "        # Both should now have the same hidden dimension\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_files, texts, tokenizer, max_text_length=100,\n",
        "                 target_sr=16000, max_duration=1.5):\n",
        "        \"\"\"\n",
        "        Dataset class that handles:\n",
        "        - File paths for batch audio processing\n",
        "        - Text sequences with tokenization\n",
        "        - Automatic audio preprocessing in batches\n",
        "        \"\"\"\n",
        "        self.audio_files = [str(f) for f in audio_files]  # Ensure string paths\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.target_sr = target_sr\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns single item with:\n",
        "        - audio_file: Path for batch processing\n",
        "        - text: Raw text string for tokenization in collate\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'audio_file': self.audio_files[idx],\n",
        "            'text': self.texts[idx],\n",
        "            'tokenizer': self.tokenizer,  # Pass tokenizer for collate_fn\n",
        "            'max_text_length': self.max_text_length\n",
        "        }\n",
        "\n",
        "def collate_fn(batch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Custom collate function that:\n",
        "    1. Processes audio files in batch using preprocess_for_rnnt_torchaudio\n",
        "    2. Tokenizes and pads text sequences\n",
        "    3. Handles audio length preservation\n",
        "    \"\"\"\n",
        "    # Extract batch components\n",
        "    audio_files = [item['audio_file'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Batch process audio files\n",
        "    audio_features, audio_lengths = preprocess_for_rnnt_torchaudio(\n",
        "        audio_files,\n",
        "        target_sr=batch[0].get('target_sr', 16000),\n",
        "        max_duration=batch[0].get('max_duration', 1.5),\n",
        "        device=device\n",
        "    )  # (B, T, 80)\n",
        "\n",
        "    target_lengths = [item['max_text_length'] - 2 for item in batch]\n",
        "\n",
        "    # Tokenize and pad texts\n",
        "    text_sequences = []\n",
        "    for item in batch:\n",
        "        encoding = item['tokenizer'].encode(item['text'])\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add special tokens\n",
        "        text_ids = [\n",
        "            item['tokenizer'].token_to_id(\"<bos>\")\n",
        "        ] + text_ids + [\n",
        "            item['tokenizer'].token_to_id(\"<eos>\")\n",
        "        ]\n",
        "\n",
        "        # Pad or truncate\n",
        "        max_len = item['max_text_length']\n",
        "        if len(text_ids) < max_len:\n",
        "            text_ids = text_ids + [item['tokenizer'].token_to_id(\"<pad>\")] * (max_len - len(text_ids))\n",
        "        else:\n",
        "            text_ids = text_ids[:max_len]\n",
        "\n",
        "        text_sequences.append(torch.tensor(text_ids, dtype=torch.int32))\n",
        "\n",
        "    target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.int32)\n",
        "    # Stack text sequences\n",
        "    text_tensor = torch.stack(text_sequences)\n",
        "\n",
        "    return {\n",
        "        'audio': audio_features.to(device),  # (B, T, 80)\n",
        "        'text': text_tensor.to(device),     # (B, U_max)\n",
        "        'target_lengths': target_lengths_tensor.to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, device=device) # (B,)\n",
        "    }"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnnt_loss(logits, targets, input_lengths, target_lengths, blank):\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "    loss = torchaudio.functional.rnnt_loss(\n",
        "        log_probs,\n",
        "        targets,\n",
        "        input_lengths,\n",
        "        target_lengths,\n",
        "        blank=blank,\n",
        "        reduction='mean'\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, device, tokenizer):\n",
        "    model.train()\n",
        "    total_loss = 100\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get batch data\n",
        "        audio = batch['audio']  # (B, T, 80)\n",
        "        text = batch['text']    # (B, U)\n",
        "\n",
        "        # Get audio lengths - must match the time dimension of the encoder output\n",
        "        input_lengths = batch['audio_lengths']\n",
        "        target_lengths = batch['target_lengths']\n",
        "\n",
        "        # Ensure input lengths don't exceed the encoder output length\n",
        "        encoder_output = model.encoder(audio)\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass input sequence and labels (excluding EOS for the decoder input)\n",
        "        # Use encoder_output directly since we already computed it\n",
        "        h_dec = model.decoder(text[:, :-1])  # (B, U-1, H)\n",
        "        logits = model.joint(encoder_output, h_dec)  # (B, T, U-1, V)\n",
        "\n",
        "        # For targets, use labels shifted right (exclude BOS)\n",
        "        targets = text[:, 1:-1].contiguous() # Remove BOS and EOS for RNN-T targets\n",
        "\n",
        "        # Compute loss\n",
        "        loss = rnnt_loss(\n",
        "            logits=logits,\n",
        "            targets=targets,\n",
        "            input_lengths=input_lengths,\n",
        "            target_lengths=target_lengths,\n",
        "            blank=0\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(\"Loss item: \", loss.item())\n",
        "\n",
        "    return total_loss / max(1, len(dataloader))"
      ],
      "metadata": {
        "id": "gXujhwb9iE4g"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "IIa9x7nMhF5c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in train_audio_files],\n",
        "    texts=train_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in test_audio_files],\n",
        "    texts=test_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        ")"
      ],
      "metadata": {
        "id": "Lhzo3JhgKWxZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Initialize model\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    # Train for one epoch\n",
        "    train_loss = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} ({(1-train_loss)/100:.4f})\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    # if args.save_dir:\n",
        "    #     os.makedirs(args.save_dir, exist_ok=True)\n",
        "    #     checkpoint_path = os.path.join(args.save_dir, f\"rnnt_epoch_{epoch+1}.pt\")\n",
        "    #     torch.save({\n",
        "    #         'epoch': epoch,\n",
        "    #         'model_state_dict': model.state_dict(),\n",
        "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #         'loss': train_loss,\n",
        "    #     }, checkpoint_path)\n",
        "    #     print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "04452275-8fdb-4544-f524-e6166765dc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: -29.1708 (0.3017)\n",
            "Train Loss: -101.1672 (1.0217)\n",
            "Train Loss: -103.0302 (1.0403)\n",
            "Train Loss: -104.3939 (1.0539)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union"
      ],
      "metadata": {
        "id": "r-8zH-OXGqFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rnnt(\n",
        "    model: torch.nn.Module,\n",
        "    audio_features: torch.Tensor,\n",
        "    tokenizer: object,\n",
        "    device: Union[str, torch.device],\n",
        "    max_decode_len: int = 100,\n",
        "    method: str = \"greedy\",\n",
        "    beam_width: int = 5,\n",
        ") -> str:\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Add batch dimension if missing (shape: [1, T, D])\n",
        "    if len(audio_features.shape) == 2:\n",
        "        audio_features = audio_features.unsqueeze(0).to(device)\n",
        "    else:\n",
        "        audio_features = audio_features.to(device)\n",
        "\n",
        "    # Initialize with BOS token\n",
        "    hyps = [[tokenizer.token_to_id(\"<bos>\")]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h_enc = model.encoder(audio_features)  # [1, T, H]\n",
        "        # print(\"h_enc shape: \", h_enc.shape)\n",
        "\n",
        "        if method == \"greedy\":\n",
        "            for _ in range(max_decode_len):\n",
        "                last_token = torch.tensor([hyps[0][-1]], device=device).unsqueeze(0)\n",
        "                # print(\"last token shape: \", last_token.shape)\n",
        "                h_dec = model.decoder(last_token)  # [1, 1, H]\n",
        "                # print(\"h_dec shape\", h_dec.shape)\n",
        "                logits = model.joint(h_enc, h_dec)  # [1, T, 1, V]\n",
        "\n",
        "                # --- FIX: Sum over time before argmax ---\n",
        "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "                scores = log_probs.squeeze(2)\n",
        "                # print(\"log probs: \", log_probs)\n",
        "                # scores = log_probs.sum(dim=1)  # [1, 1, V]\n",
        "                # print(\"scores: \", scores)\n",
        "                next_token = scores.argmax(-1)[0, -1].item()  # Now a scalar\n",
        "                # ---------------------------------------\n",
        "                # print(\"Next token: \", next_token)\n",
        "                if next_token == tokenizer.token_to_id(\"<eos>\"):\n",
        "                    break\n",
        "                if next_token != 0:  # Skip blank tokens\n",
        "                    hyps[0].append(next_token)\n",
        "\n",
        "            print(\"hyps: \", hyps)\n",
        "            return tokenizer.decode(hyps[0][1:])\n",
        "\n",
        "        elif method == \"beam_search\":\n",
        "            beam_hyps = hyps\n",
        "            beam_scores = torch.zeros(1, device=device)\n",
        "\n",
        "            for _ in range(max_decode_len):\n",
        "                candidates = []\n",
        "                for i, hyp in enumerate(beam_hyps):\n",
        "                    last_token = torch.tensor([hyp[-1]], device=device).unsqueeze(0)\n",
        "                    h_dec = model.decoder(last_token)\n",
        "                    logits = model.joint(h_enc, h_dec)  # [1, T, 1, V]\n",
        "                    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "                    # --- FIX: Sum over time before top-k ---\n",
        "                    scores = log_probs.sum(dim=1)  # [1, 1, V]\n",
        "                    topk_scores, topk_tokens = scores.topk(beam_width, dim=-1)\n",
        "                    # ---------------------------------------\n",
        "\n",
        "                    for j in range(beam_width):\n",
        "                        new_hyp = hyp + [topk_tokens[0, 0, j].item()]\n",
        "                        new_score = beam_scores[i] + topk_scores[0, 0, j]\n",
        "                        candidates.append((new_hyp, new_score))\n",
        "\n",
        "                candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "                beam_hyps = [x[0] for x in candidates[:beam_width]]\n",
        "                beam_scores = torch.tensor([x[1] for x in candidates[:beam_width]], device=device)\n",
        "\n",
        "                if all(h[-1] == tokenizer.token_to_id(\"<eos>\") for h in beam_hyps):\n",
        "                    break\n",
        "\n",
        "            return tokenizer.decode(beam_hyps[0][1:])\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown decoding method: {method}\")\n"
      ],
      "metadata": {
        "id": "kT0pF69O47CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model, tokenizer, and features\n",
        "model = model.eval()  # Your RNN-T model\n",
        "audio_features, _ = preprocess_for_rnnt_torchaudio([os.path.join(audio_directory, test_audio_files[-1])])  # Shape: [T, D]\n",
        "\n",
        "print(\"Audio shape\", audio_features.shape)\n",
        "# Greedy decoding (fast)\n",
        "text_greedy = predict_rnnt(\n",
        "    model, audio_features, tokenizer, \"cuda\", method=\"greedy\"\n",
        ")\n",
        "\n",
        "# Beam search (better accuracy)\n",
        "text_beam = predict_rnnt(\n",
        "    model, audio_features, tokenizer, \"cuda\", method=\"beam_search\", beam_width=5\n",
        ")\n",
        "\n",
        "print(f\"Greedy: {text_greedy}\\nBeam Search: {text_beam}\")"
      ],
      "metadata": {
        "id": "3WcBnC1yqvIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}