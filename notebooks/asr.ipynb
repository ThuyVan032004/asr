{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil"
      ],
      "metadata": {
        "id": "eRWwuyT8Bjbk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source_folder = 'checkpoints'\n",
        "# destination_folder = 'datasets/checkpoints'  # Create 'checkpoints' inside 'datasets'\n",
        "\n",
        "# shutil.move(source_folder, destination_folder)"
      ],
      "metadata": {
        "id": "0PQAOeZRBnhI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "Mg_YAjQ9ttKS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "id": "Gz0vNVrDnO37",
        "outputId": "f50871cc-2c3c-4520-fd25-e4d8560fcb72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-train.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df = dev_df"
      ],
      "metadata": {
        "id": "SPCU8pwmKP0z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[2000:2050]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "outputId": "70c96985-cdc7-4ab8-d9b7-75293f485b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-train'\n",
        "\n",
        "train_size = int(0.8 * len(dev_df))\n",
        "train_df = dev_df.iloc[:train_size]\n",
        "test_df = dev_df.iloc[train_size+1:]\n",
        "\n",
        "train_audio_files = [os.path.join(f) for f in train_df['filename']]\n",
        "train_texts = train_df['text'].tolist()\n",
        "\n",
        "test_audio_files = [os.path.join(f) for f in test_df['filename']]\n",
        "test_texts = test_df['text'].tolist()"
      ],
      "metadata": {
        "id": "mzKLy1FmUBqo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<blank>\"],\n",
        "    vocab_size=40000,  # Increased for better word coverage\n",
        "    min_frequency=2,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in train_texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "RQhYjuBSMj0v",
        "outputId": "3e081ec8-89da-46fc-fa81-3114a12d2337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39771"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install noisereduce"
      ],
      "metadata": {
        "id": "17IXsOsYyadF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import noisereduce as nr\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IwEcJmCK_o1j"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_silence(audio, sr, top_db=20, frame_length=2048, hop_length=512):\n",
        "    \"\"\"Remove silent segments from audio using librosa's trim function\"\"\"\n",
        "    # Trim leading and trailing silence\n",
        "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db,\n",
        "                                          frame_length=frame_length,\n",
        "                                          hop_length=hop_length)\n",
        "    return trimmed_audio"
      ],
      "metadata": {
        "id": "lT8wUa8Hq4O2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_paths, target_sr=16000, max_duration=5, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing pipeline for RNN-T\n",
        "    Returns:\n",
        "        log_mel: (batch_size, T, 80) tensor of log Mel spectrograms\n",
        "        lengths: (batch_size,) tensor of actual lengths in frames\n",
        "    \"\"\"\n",
        "    # Initialize mel transform once\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        win_length=400,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_specs = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    # Calculate max_len in frames based on max_duration\n",
        "    max_len = int(max_duration * target_sr / 160)  # hop_length is 160\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Load audio with original sample rate\n",
        "        audio_data, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # ===== SILENCE REMOVAL =====\n",
        "        original_duration = len(audio_data) / sr\n",
        "        audio_data = remove_silence(audio_data, sr)\n",
        "        new_duration = len(audio_data) / sr\n",
        "        silence_removed = original_duration - new_duration\n",
        "\n",
        "        # ===== NOISE REDUCTION =====\n",
        "        # Only attempt noise reduction if we have enough audio left\n",
        "        if len(audio_data) > 0.2 * sr:  # At least 200ms remaining\n",
        "            # Estimate noise from non-silent portions (first 100ms after silence removal)\n",
        "            noise_samples = int(0.1 * sr)\n",
        "            noise_profile = audio_data[:min(noise_samples, len(audio_data))]\n",
        "\n",
        "            reduced_noise = nr.reduce_noise(\n",
        "                y=audio_data,\n",
        "                sr=sr,\n",
        "                y_noise=noise_profile,\n",
        "                stationary=False,\n",
        "                prop_decrease=0.8\n",
        "            )\n",
        "        else:\n",
        "            reduced_noise = audio_data  # Skip if too short after silence removal\n",
        "\n",
        "        # ===== VOLUME NORMALIZATION =====\n",
        "        rms = np.sqrt(np.mean(reduced_noise**2))\n",
        "        current_dBFS = 20 * np.log10(max(rms, 1e-10))\n",
        "        target_dBFS = -20\n",
        "        gain = min(target_dBFS - current_dBFS, 20)\n",
        "\n",
        "        if gain > 1:\n",
        "            amplified_audio = reduced_noise * (10 ** (gain / 20))\n",
        "            amplified_audio = np.clip(amplified_audio, -1.0, 1.0)\n",
        "        else:\n",
        "            amplified_audio = reduced_noise\n",
        "\n",
        "        # ===== RESAMPLING =====\n",
        "        target_sr = 16000\n",
        "        if sr != target_sr:\n",
        "            resampled_audio = librosa.resample(amplified_audio, orig_sr=sr, target_sr=target_sr)\n",
        "        else:\n",
        "            resampled_audio = amplified_audio\n",
        "\n",
        "        sf.write(file_path, resampled_audio, target_sr, format='mp3')\n",
        "        audio_tensor, sr = torchaudio.load(file_path)\n",
        "        audio_tensor = audio_tensor.to(device)\n",
        "        # 3. Mel transform\n",
        "        mel_spec = mel_transform(audio_tensor)  # (channels, n_mels, time)\n",
        "        log_mel = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
        "        log_mel = (log_mel - log_mel.min()) / (log_mel.max() - log_mel.min())\n",
        "        # Permute to (channels, time, n_mels)\n",
        "        log_mel = log_mel.permute(0, 2, 1)\n",
        "        # 4. Padding/trimming\n",
        "        if log_mel.size(1) > max_len:\n",
        "            log_mel = log_mel[:, :max_len+1, :]\n",
        "        else:\n",
        "            pad_amount = max_len + 1 - log_mel.size(1)\n",
        "            log_mel = torch.nn.functional.pad(log_mel, (0, 0, 0, pad_amount))\n",
        "\n",
        "        mel_specs.append(log_mel.squeeze(0))  # Remove channel dimension if single channel\n",
        "        audio_lengths.append(log_mel.size(1))\n",
        "\n",
        "    # Stack into batch (batch_size, T, n_mels)\n",
        "    mel_specs = torch.stack(mel_specs)\n",
        "    audio_lengths = torch.tensor(audio_lengths, dtype=torch.int32)\n",
        "\n",
        "    return mel_specs, audio_lengths"
      ],
      "metadata": {
        "id": "MTcU-suGxili"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-DcGDWbuMZ7v"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_mel, audio_lengths = preprocess_for_rnnt_torchaudio(['cv-valid-test/cv-valid-test/sample-000001.mp3'], device=device)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# # Convert the PyTorch tensor to a NumPy array\n",
        "# librosa.display.specshow(\n",
        "#     log_mel.cpu().numpy()[0], # Access the first element if it's a batch of size 1\n",
        "#     # sr=16000,\n",
        "#     x_axis='time',\n",
        "#     y_axis='mel',\n",
        "#     fmax=8000,\n",
        "# )\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.title('Mel Spectrogram')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "o1DX9a-fLcKR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, input_dim)\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)   # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # y: (B, U)\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        y = self.dropout(y)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        # Use a more robust approach with a multi-layer network\n",
        "        self.joint = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "\n",
        "        # Get batch size and sequence lengths\n",
        "        batch_size = h_enc.size(0)\n",
        "        T = h_enc.size(1)  # Audio sequence length\n",
        "        U = h_dec.size(1)  # Text sequence length\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "\n",
        "        # Expand to create the full cartesian product for alignment\n",
        "        h_enc = h_enc.expand(-1, -1, U, -1)  # (B, T, U, H)\n",
        "        h_dec = h_dec.expand(-1, T, -1, -1)  # (B, T, U, H)\n",
        "\n",
        "        # Concatenate features\n",
        "        joint = torch.cat([h_enc, h_dec], dim=-1)  # (B, T, U, 2H)\n",
        "\n",
        "        # Apply joint network to get logits\n",
        "        logits = self.joint(joint)  # (B, T, U, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(decoder_dim, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: (B, T, features), y: (B, U)\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "\n",
        "        # Both should now have the same hidden dimension\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_files, texts, tokenizer, max_text_length=100,\n",
        "                 target_sr=16000, max_duration=5):\n",
        "        \"\"\"\n",
        "        Dataset class that handles:\n",
        "        - File paths for batch audio processing\n",
        "        - Text sequences with tokenization\n",
        "        - Automatic audio preprocessing in batches\n",
        "        \"\"\"\n",
        "        self.audio_files = [str(f) for f in audio_files]  # Ensure string paths\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.target_sr = target_sr\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns single item with:\n",
        "        - audio_file: Path for batch processing\n",
        "        - text: Raw text string for tokenization in collate\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'audio_file': self.audio_files[idx],\n",
        "            'text': self.texts[idx],\n",
        "            'tokenizer': self.tokenizer,  # Pass tokenizer for collate_fn\n",
        "            'max_text_length': self.max_text_length\n",
        "        }\n",
        "\n",
        "def collate_fn(batch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Custom collate function that:\n",
        "    1. Processes audio files in batch using preprocess_for_rnnt_torchaudio\n",
        "    2. Tokenizes and pads text sequences\n",
        "    3. Handles audio length preservation\n",
        "    \"\"\"\n",
        "    # Extract batch components\n",
        "    audio_files = [item['audio_file'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Batch process audio files\n",
        "    audio_features, audio_lengths = preprocess_for_rnnt_torchaudio(\n",
        "        audio_files,\n",
        "        target_sr=batch[0].get('target_sr', 16000),\n",
        "        max_duration=batch[0].get('max_duration', 5),\n",
        "        device=device\n",
        "    )  # (B, T, 80)\n",
        "\n",
        "    target_lengths = [item['max_text_length'] - 2 for item in batch]\n",
        "\n",
        "    # Tokenize and pad texts\n",
        "    text_sequences = []\n",
        "    for item in batch:\n",
        "        encoding = item['tokenizer'].encode(item['text'])\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add special tokens\n",
        "        text_ids = [\n",
        "            item['tokenizer'].token_to_id(\"<bos>\")\n",
        "        ] + text_ids + [\n",
        "            item['tokenizer'].token_to_id(\"<eos>\")\n",
        "        ]\n",
        "\n",
        "        # Pad or truncate\n",
        "        max_len = item['max_text_length']\n",
        "        if len(text_ids) < max_len:\n",
        "            text_ids = text_ids + [item['tokenizer'].token_to_id(\"<pad>\")] * (max_len - len(text_ids))\n",
        "        else:\n",
        "            text_ids = text_ids[:max_len]\n",
        "\n",
        "        text_sequences.append(torch.tensor(text_ids, dtype=torch.int32))\n",
        "\n",
        "    target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.int32)\n",
        "    # Stack text sequences\n",
        "    text_tensor = torch.stack(text_sequences)\n",
        "\n",
        "    return {\n",
        "        'audio': audio_features.to(device),  # (B, T, 80)\n",
        "        'text': text_tensor.to(device),     # (B, U_max)\n",
        "        'target_lengths': target_lengths_tensor.to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, device=device) # (B,)\n",
        "    }"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision=True):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    # Enable mixed precision for faster training and lower memory usage\n",
        "    scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get batch data\n",
        "        audio = batch['audio']  # (B, T, 80)\n",
        "        text = batch['text']    # (B, U)\n",
        "\n",
        "        # Get lengths\n",
        "        input_lengths = batch['audio_lengths']\n",
        "        target_lengths = batch['target_lengths']\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mixed_precision:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Forward pass - encoder\n",
        "                encoder_output = model.encoder(audio)\n",
        "\n",
        "                # Forward pass - decoder and joint\n",
        "                h_dec = model.decoder(text[:, :-1])  # (B, U-1, H)\n",
        "                logits = model.joint(encoder_output, h_dec)  # (B, T, U-1, V)\n",
        "\n",
        "                # For targets, use labels shifted right (exclude BOS)\n",
        "                targets = text[:, 1:-1].contiguous()  # Remove only BOS for targets\n",
        "\n",
        "                # Compute loss\n",
        "                blank_id = tokenizer.token_to_id('<blank>')\n",
        "                loss = torchaudio.functional.rnnt_loss(\n",
        "                    logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                    targets=targets,\n",
        "                    logit_lengths=input_lengths,\n",
        "                    target_lengths=target_lengths,\n",
        "                    blank=blank_id,\n",
        "                    reduction='mean'\n",
        "                )\n",
        "\n",
        "            # Backward pass with scaled gradients\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Lower clip threshold\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Standard training without mixed precision\n",
        "            encoder_output = model.encoder(audio)\n",
        "            h_dec = model.decoder(text[:, :-1])\n",
        "            logits = model.joint(encoder_output, h_dec)\n",
        "\n",
        "            targets = text[:, 1:-1].contiguous()\n",
        "\n",
        "            blank_id = tokenizer.token_to_id('<blank>')\n",
        "            loss = torchaudio.functional.rnnt_loss(\n",
        "                logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                targets=targets,\n",
        "                logit_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=blank_id,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update learning rate with scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    return total_loss / steps"
      ],
      "metadata": {
        "id": "AbKbvmzwtkc3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "IIa9x7nMhF5c"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in train_audio_files],\n",
        "    texts=train_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in test_audio_files],\n",
        "    texts=test_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        ")"
      ],
      "metadata": {
        "id": "Lhzo3JhgKWxZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_path = 'checkpoints/rnnt_epoch_94.pt'\n",
        "# checkpoint = torch.load(checkpoint_path)"
      ],
      "metadata": {
        "id": "z0KRKbxDCIdV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install jiwer"
      ],
      "metadata": {
        "id": "Xj1DFE1kL3RZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union"
      ],
      "metadata": {
        "id": "szj-GdzWKVFm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device, tokenizer):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            audio = batch['audio']\n",
        "            text = batch['text']\n",
        "            input_lengths = batch['audio_lengths']\n",
        "            target_lengths = batch['target_lengths']\n",
        "\n",
        "            # Forward pass\n",
        "            encoder_output = model.encoder(audio)\n",
        "            h_dec = model.decoder(text[:, :-1])\n",
        "            logits = model.joint(encoder_output, h_dec)\n",
        "\n",
        "            # Prepare targets (exclude BOS)\n",
        "            targets = text[:, 1:-1].contiguous()\n",
        "\n",
        "            # Compute loss\n",
        "            blank_id = tokenizer.token_to_id('<blank>')\n",
        "            loss = torchaudio.functional.rnnt_loss(\n",
        "                logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                targets=targets,\n",
        "                logit_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=blank_id,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "    return total_loss / steps"
      ],
      "metadata": {
        "id": "yCPPSxgVYpy7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-4,\n",
        "    weight_decay=1e-6,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "    # Train for one epoch\n",
        "    train_loss = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=None,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        mixed_precision=True\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss = evaluate(\n",
        "        model=model,\n",
        "        dataloader=test_loader,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(save_dir, f\"rnnt_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'test_loss': test_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020",
        "collapsed": true,
        "outputId": "1d2bdfbd-6a3d-481b-bb7b-4dc4b2759917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 7.35 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.35 GiB is free. Process 71674 has 11.39 GiB memory in use. Of the allocated memory 11.21 GiB is allocated by PyTorch, and 40.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e06993cc8749>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-c8edfe3aee65>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mblank_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<blank>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 loss = torchaudio.functional.rnnt_loss(\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mrnnt_loss\u001b[0;34m(logits, targets, logit_lengths, target_lengths, blank, clamp, reduction, fused_log_softmax)\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0mblank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m     costs, _ = torch.ops.torchaudio.rnnt_loss(\n\u001b[0m\u001b[1;32m   1806\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.35 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.35 GiB is free. Process 71674 has 11.39 GiB memory in use. Of the allocated memory 11.21 GiB is allocated by PyTorch, and 40.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import torch\n",
        "\n",
        "# # Specify the directory where checkpoints are saved\n",
        "# save_dir = 'checkpoints'\n",
        "\n",
        "# # Get the last checkpoint file\n",
        "# last_checkpoint_file = 'rnnt_epoch_94.pt'\n",
        "\n",
        "# # Load the checkpoint\n",
        "# checkpoint_path = os.path.join(save_dir, last_checkpoint_file)\n",
        "# checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# # Create a new instance of your model (if needed)\n",
        "# model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "\n",
        "# # Load the model state dictionary from the checkpoint\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# # Load the optimizer state dictionary (optional)\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# # Set the model to evaluation mode\n",
        "\n",
        "# print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "# print(f\"Loss: {checkpoint['loss']}\")"
      ],
      "metadata": {
        "id": "2vJUeqkrgVvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r checkpoints.zip checkpoints"
      ],
      "metadata": {
        "id": "Qq77BA25Oiko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoints.zip')"
      ],
      "metadata": {
        "id": "i7MLURLXOmJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}