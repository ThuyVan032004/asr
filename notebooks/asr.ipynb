{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "Mg_YAjQ9ttKS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "35ecdd47-32cf-4ca8-90e5-6a149a41040f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-train.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df[:10000]"
      ],
      "metadata": {
        "id": "SPCU8pwmKP0z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[2000:2050]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4f29ec-a5f7-4ff6-c54f-9cca24a55120"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-train'\n",
        "\n",
        "train_size = int(0.8 * len(dev_df))\n",
        "train_df = dev_df.iloc[:train_size]\n",
        "test_df = dev_df.iloc[train_size+1:]\n",
        "\n",
        "train_audio_files = [os.path.join(f) for f in train_df['filename']]\n",
        "train_texts = train_df['text'].tolist()\n",
        "\n",
        "test_audio_files = [os.path.join(f) for f in test_df['filename']]\n",
        "test_texts = test_df['text'].tolist()"
      ],
      "metadata": {
        "id": "mzKLy1FmUBqo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.info()"
      ],
      "metadata": {
        "id": "MbdikRp0M3x6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<blank>\"],\n",
        "    vocab_size=16000,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in train_texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "RQhYjuBSMj0v"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install noisereduce"
      ],
      "metadata": {
        "id": "17IXsOsYyadF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import noisereduce as nr\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IwEcJmCK_o1j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_silence(audio, sr, top_db=20, frame_length=2048, hop_length=512):\n",
        "    \"\"\"Remove silent segments from audio using librosa's trim function\"\"\"\n",
        "    # Trim leading and trailing silence\n",
        "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db,\n",
        "                                          frame_length=frame_length,\n",
        "                                          hop_length=hop_length)\n",
        "    return trimmed_audio"
      ],
      "metadata": {
        "id": "lT8wUa8Hq4O2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_paths, target_sr=16000, max_duration=5, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing pipeline for RNN-T\n",
        "    Returns:\n",
        "        log_mel: (batch_size, T, 80) tensor of log Mel spectrograms\n",
        "        lengths: (batch_size,) tensor of actual lengths in frames\n",
        "    \"\"\"\n",
        "    # Initialize mel transform once\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        win_length=400,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_specs = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    # Calculate max_len in frames based on max_duration\n",
        "    max_len = int(max_duration * target_sr / 160)  # hop_length is 160\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Load audio with original sample rate\n",
        "        audio_data, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # ===== SILENCE REMOVAL =====\n",
        "        original_duration = len(audio_data) / sr\n",
        "        audio_data = remove_silence(audio_data, sr)\n",
        "        new_duration = len(audio_data) / sr\n",
        "        silence_removed = original_duration - new_duration\n",
        "\n",
        "        # ===== NOISE REDUCTION =====\n",
        "        # Only attempt noise reduction if we have enough audio left\n",
        "        if len(audio_data) > 0.2 * sr:  # At least 200ms remaining\n",
        "            # Estimate noise from non-silent portions (first 100ms after silence removal)\n",
        "            noise_samples = int(0.1 * sr)\n",
        "            noise_profile = audio_data[:min(noise_samples, len(audio_data))]\n",
        "\n",
        "            reduced_noise = nr.reduce_noise(\n",
        "                y=audio_data,\n",
        "                sr=sr,\n",
        "                y_noise=noise_profile,\n",
        "                stationary=False,\n",
        "                prop_decrease=0.8\n",
        "            )\n",
        "        else:\n",
        "            reduced_noise = audio_data  # Skip if too short after silence removal\n",
        "\n",
        "        # ===== VOLUME NORMALIZATION =====\n",
        "        rms = np.sqrt(np.mean(reduced_noise**2))\n",
        "        current_dBFS = 20 * np.log10(max(rms, 1e-10))\n",
        "        target_dBFS = -20\n",
        "        gain = min(target_dBFS - current_dBFS, 20)\n",
        "\n",
        "        if gain > 1:\n",
        "            amplified_audio = reduced_noise * (10 ** (gain / 20))\n",
        "            amplified_audio = np.clip(amplified_audio, -1.0, 1.0)\n",
        "        else:\n",
        "            amplified_audio = reduced_noise\n",
        "\n",
        "        # ===== RESAMPLING =====\n",
        "        target_sr = 16000\n",
        "        if sr != target_sr:\n",
        "            resampled_audio = librosa.resample(amplified_audio, orig_sr=sr, target_sr=target_sr)\n",
        "        else:\n",
        "            resampled_audio = amplified_audio\n",
        "\n",
        "        sf.write(file_path, resampled_audio, target_sr, format='mp3')\n",
        "        audio_tensor, sr = torchaudio.load(file_path)\n",
        "        audio_tensor = audio_tensor.to(device)\n",
        "        # 3. Mel transform\n",
        "        mel_spec = mel_transform(audio_tensor)  # (channels, n_mels, time)\n",
        "        log_mel = torch.log(mel_spec + 1e-8)\n",
        "        mean = log_mel.mean(dim=2, keepdim=True)\n",
        "        std = log_mel.std(dim=2, keepdim=True) + 1e-8\n",
        "        log_mel = (log_mel - mean) / std\n",
        "        # Permute to (channels, time, n_mels)\n",
        "        log_mel = log_mel.permute(0, 2, 1)\n",
        "        # 4. Padding/trimming\n",
        "        if log_mel.size(1) > max_len:\n",
        "            log_mel = log_mel[:, :max_len+1, :]\n",
        "        else:\n",
        "            pad_amount = max_len + 1 - log_mel.size(1)\n",
        "            log_mel = torch.nn.functional.pad(log_mel, (0, 0, 0, pad_amount))\n",
        "\n",
        "        mel_specs.append(log_mel.squeeze(0))  # Remove channel dimension if single channel\n",
        "        audio_lengths.append(log_mel.size(1))\n",
        "\n",
        "    # Stack into batch (batch_size, T, n_mels)\n",
        "    mel_specs = torch.stack(mel_specs)\n",
        "    audio_lengths = torch.tensor(audio_lengths, dtype=torch.int32)\n",
        "\n",
        "    return mel_specs, audio_lengths"
      ],
      "metadata": {
        "id": "MTcU-suGxili"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, input_dim)\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)   # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # y: (B, U)\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        y = self.dropout(y)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        # Use a more robust approach with a multi-layer network\n",
        "        self.joint = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "\n",
        "        # Get batch size and sequence lengths\n",
        "        batch_size = h_enc.size(0)\n",
        "        T = h_enc.size(1)  # Audio sequence length\n",
        "        U = h_dec.size(1)  # Text sequence length\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "\n",
        "        # Expand to create the full cartesian product for alignment\n",
        "        h_enc = h_enc.expand(-1, -1, U, -1)  # (B, T, U, H)\n",
        "        h_dec = h_dec.expand(-1, T, -1, -1)  # (B, T, U, H)\n",
        "\n",
        "        # Concatenate features\n",
        "        joint = torch.cat([h_enc, h_dec], dim=-1)  # (B, T, U, 2H)\n",
        "\n",
        "        # Apply joint network to get logits\n",
        "        logits = self.joint(joint)  # (B, T, U, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(decoder_dim, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: (B, T, features), y: (B, U)\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "\n",
        "        # Both should now have the same hidden dimension\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_files, texts, tokenizer, max_text_length=100,\n",
        "                 target_sr=16000, max_duration=2):\n",
        "        \"\"\"\n",
        "        Dataset class that handles:\n",
        "        - File paths for batch audio processing\n",
        "        - Text sequences with tokenization\n",
        "        - Automatic audio preprocessing in batches\n",
        "        \"\"\"\n",
        "        self.audio_files = [str(f) for f in audio_files]  # Ensure string paths\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.target_sr = target_sr\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns single item with:\n",
        "        - audio_file: Path for batch processing\n",
        "        - text: Raw text string for tokenization in collate\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'audio_file': self.audio_files[idx],\n",
        "            'text': self.texts[idx],\n",
        "            'tokenizer': self.tokenizer,  # Pass tokenizer for collate_fn\n",
        "            'max_text_length': self.max_text_length\n",
        "        }\n",
        "\n",
        "def collate_fn(batch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Custom collate function that:\n",
        "    1. Processes audio files in batch using preprocess_for_rnnt_torchaudio\n",
        "    2. Tokenizes and pads text sequences\n",
        "    3. Handles audio length preservation\n",
        "    \"\"\"\n",
        "    # Extract batch components\n",
        "    audio_files = [item['audio_file'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Batch process audio files\n",
        "    audio_features, audio_lengths = preprocess_for_rnnt_torchaudio(\n",
        "        audio_files,\n",
        "        target_sr=batch[0].get('target_sr', 16000),\n",
        "        max_duration=batch[0].get('max_duration', 2),\n",
        "        device=device\n",
        "    )  # (B, T, 80)\n",
        "\n",
        "    target_lengths = [item['max_text_length'] - 2 for item in batch]\n",
        "\n",
        "    # Tokenize and pad texts\n",
        "    text_sequences = []\n",
        "    for item in batch:\n",
        "        encoding = item['tokenizer'].encode(item['text'])\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add special tokens\n",
        "        text_ids = [\n",
        "            item['tokenizer'].token_to_id(\"<bos>\")\n",
        "        ] + text_ids + [\n",
        "            item['tokenizer'].token_to_id(\"<eos>\")\n",
        "        ]\n",
        "\n",
        "        # Pad or truncate\n",
        "        max_len = item['max_text_length']\n",
        "        if len(text_ids) < max_len:\n",
        "            text_ids = text_ids + [item['tokenizer'].token_to_id(\"<pad>\")] * (max_len - len(text_ids))\n",
        "        else:\n",
        "            text_ids = text_ids[:max_len]\n",
        "\n",
        "        text_sequences.append(torch.tensor(text_ids, dtype=torch.int32))\n",
        "\n",
        "    target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.int32)\n",
        "    # Stack text sequences\n",
        "    text_tensor = torch.stack(text_sequences)\n",
        "\n",
        "    return {\n",
        "        'audio': audio_features.to(device),  # (B, T, 80)\n",
        "        'text': text_tensor.to(device),     # (B, U_max)\n",
        "        'target_lengths': target_lengths_tensor.to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, device=device) # (B,)\n",
        "    }"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision=True):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    # Enable mixed precision for faster training and lower memory usage\n",
        "    scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get batch data\n",
        "        audio = batch['audio']  # (B, T, 80)\n",
        "        text = batch['text']    # (B, U)\n",
        "\n",
        "        # Get lengths\n",
        "        input_lengths = batch['audio_lengths']\n",
        "        target_lengths = batch['target_lengths']\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mixed_precision:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Forward pass - encoder\n",
        "                encoder_output = model.encoder(audio)\n",
        "\n",
        "                # Forward pass - decoder and joint\n",
        "                h_dec = model.decoder(text[:, :-1])  # (B, U-1, H)\n",
        "                logits = model.joint(encoder_output, h_dec)  # (B, T, U-1, V)\n",
        "\n",
        "                # For targets, use labels shifted right (exclude BOS)\n",
        "                targets = text[:, 1:-1].contiguous()  # Remove only BOS for targets\n",
        "\n",
        "                # Compute loss\n",
        "                blank_id = tokenizer.token_to_id('<blank>')\n",
        "                loss = torchaudio.functional.rnnt_loss(\n",
        "                    logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                    targets=targets,\n",
        "                    logit_lengths=input_lengths,\n",
        "                    target_lengths=target_lengths,\n",
        "                    blank=blank_id,\n",
        "                    reduction='mean'\n",
        "                )\n",
        "\n",
        "            # Backward pass with scaled gradients\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Lower clip threshold\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Standard training without mixed precision\n",
        "            encoder_output = model.encoder(audio)\n",
        "            h_dec = model.decoder(text[:, :-1])\n",
        "            logits = model.joint(encoder_output, h_dec)\n",
        "\n",
        "            targets = text[:, 1:-1].contiguous()\n",
        "\n",
        "            blank_id = tokenizer.token_to_id('<blank>')\n",
        "            loss = torchaudio.functional.rnnt_loss(\n",
        "                logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                targets=targets,\n",
        "                logit_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=blank_id,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update learning rate with scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    return total_loss / steps"
      ],
      "metadata": {
        "id": "AbKbvmzwtkc3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "IIa9x7nMhF5c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in train_audio_files],\n",
        "    texts=train_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in test_audio_files],\n",
        "    texts=test_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        ")"
      ],
      "metadata": {
        "id": "Lhzo3JhgKWxZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5,  # L2 regularization\n",
        "        betas=(0.9, 0.999))\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "#         optimizer,\n",
        "#         max_lr=5e-4,\n",
        "#         steps_per_epoch=len(train_loader),\n",
        "#         epochs=100,\n",
        "#         pct_start=0.2,  # Warm-up for first 10% of training\n",
        "#         div_factor=10,  # Initial LR = max_lr/div_factor\n",
        "#         final_div_factor=100,  # Final LR = initial_lr/final_div_factor\n",
        "#     )\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    # Train for one epoch\n",
        "    train_loss = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=None,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        mixed_precision=True\n",
        "    )\n",
        "\n",
        "    print(f\"Train Loss {epoch}: {train_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if save_dir: # Use save_dir instead of args.save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(save_dir, f\"rnnt_epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020",
        "collapsed": true,
        "outputId": "c38008b0-f5c5-4c47-fc3e-0e4fee2a06d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss 0: 82.4463\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_1.pt\n",
            "Train Loss 1: 68.5519\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_2.pt\n",
            "Train Loss 2: 68.5210\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_3.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-84d795d6c7e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-c8edfe3aee65>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Backward pass with scaled gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Lower clip threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union"
      ],
      "metadata": {
        "id": "r-8zH-OXGqFI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rnnt_beam_search(\n",
        "    model: torch.nn.Module,\n",
        "    audio_features: torch.Tensor,\n",
        "    tokenizer: object,\n",
        "    device: Union[str, torch.device],\n",
        "    beam_width: int = 5,\n",
        "    max_decode_len: int = 100,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Performs beam search decoding for an RNN-T model.\n",
        "\n",
        "    Args:\n",
        "        model: The RNN-T model\n",
        "        audio_features: The audio features tensor\n",
        "        tokenizer: The tokenizer object\n",
        "        device: The device to run inference on\n",
        "        beam_width: Width of the beam search\n",
        "        max_decode_len: Maximum decoding length\n",
        "\n",
        "    Returns:\n",
        "        The best decoded transcript\n",
        "    \"\"\"\n",
        "    # Add batch dimension if missing (shape: [1, T, D])\n",
        "    if len(audio_features.shape) == 2:\n",
        "        audio_features = audio_features.unsqueeze(0).to(device)\n",
        "    else:\n",
        "        audio_features = audio_features.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode the audio features\n",
        "        h_enc = model.encoder(audio_features)  # [1, T, H]\n",
        "\n",
        "        # Initialize the beam\n",
        "        # Each beam element: (negative log prob, sequence, decoder state)\n",
        "        bos_token = tokenizer.token_to_id(\"<bos>\")\n",
        "        eos_token = tokenizer.token_to_id(\"<eos>\")\n",
        "        blank_token = tokenizer.token_to_id(\"<blank>\")\n",
        "\n",
        "        # Initialize beam with just the beginning token\n",
        "        current_idx = torch.tensor([[bos_token]], device=device)\n",
        "        initial_decoder_output = model.decoder(current_idx)  # [1, 1, H]\n",
        "\n",
        "        # Start with a single beam\n",
        "        beams = [(0.0, [bos_token], initial_decoder_output)]\n",
        "        finished_beams = []\n",
        "\n",
        "        # Main beam search loop\n",
        "        for step in range(max_decode_len):\n",
        "            candidates = []\n",
        "\n",
        "            # Expand each current beam\n",
        "            for log_prob, sequence, decoder_output in beams:\n",
        "                # Skip if sequence already ended\n",
        "                if sequence[-1] == eos_token:\n",
        "                    finished_beams.append((log_prob, sequence))\n",
        "                    continue\n",
        "\n",
        "                # Run joint network on the encoded audio and last decoder output\n",
        "                logits = model.joint(h_enc, decoder_output)  # [1, T, 1, V]\n",
        "\n",
        "                # Sum over time dimension for alignment scores\n",
        "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "                scores = log_probs.sum(dim=1).squeeze(1)  # [1, V]\n",
        "\n",
        "                # Get top-k tokens\n",
        "                top_log_probs, top_indices = scores.topk(beam_width)\n",
        "\n",
        "                # Add candidates to our list\n",
        "                for i in range(beam_width):\n",
        "                    token_id = top_indices[0, i].item()\n",
        "                    token_log_prob = top_log_probs[0, i].item()\n",
        "\n",
        "                    # Skip blank tokens in final output but consider them for alignment\n",
        "                    if token_id == blank_token:\n",
        "                        candidates.append((log_prob + token_log_prob, sequence.copy(), decoder_output))\n",
        "                    else:\n",
        "                        new_sequence = sequence.copy() + [token_id]\n",
        "\n",
        "                        # Update decoder state with new token\n",
        "                        new_token = torch.tensor([[token_id]], device=device)\n",
        "                        new_decoder_output = model.decoder(new_token)\n",
        "\n",
        "                        candidates.append((log_prob + token_log_prob, new_sequence, new_decoder_output))\n",
        "\n",
        "            # Keep only the best beam_width candidates\n",
        "            candidates.sort(key=lambda x: x[0], reverse=True)  # Sort by log probability (higher is better)\n",
        "            beams = candidates[:beam_width]\n",
        "\n",
        "            # Early stopping if all beams have finished\n",
        "            if all(beam[1][-1] == eos_token for beam in beams):\n",
        "                break\n",
        "\n",
        "        # Add any unfinished beams to finished_beams\n",
        "        for log_prob, sequence, _ in beams:\n",
        "            if sequence[-1] != eos_token:\n",
        "                finished_beams.append((log_prob, sequence))\n",
        "\n",
        "        best_sequence = None\n",
        "        # print(\"Number of beams: \", finished_beams)\n",
        "        # Select the most likely beam\n",
        "        if finished_beams:\n",
        "            best_beam = max(finished_beams, key=lambda x: x[0])\n",
        "            best_sequence = best_beam[1]\n",
        "        else:\n",
        "            best_sequence = beams[0][1]  # Best unfinished beam\n",
        "\n",
        "        # Remove special tokens (BOS, EOS) for the final output\n",
        "        filtered_sequence = [token for token in best_sequence\n",
        "                            if token != bos_token and token != eos_token]\n",
        "\n",
        "        return tokenizer.decode(filtered_sequence)"
      ],
      "metadata": {
        "id": "t7iEpIvbLUOr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Specify the directory where checkpoints are saved\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Get the last checkpoint file\n",
        "last_checkpoint_file = 'rnnt_epoch_3.pt'\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint_path = os.path.join(save_dir, last_checkpoint_file)\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Create a new instance of your model (if needed)\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "\n",
        "# Load the model state dictionary from the checkpoint\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Load the optimizer state dictionary (optional)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "\n",
        "print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "print(f\"Loss: {checkpoint['loss']}\")"
      ],
      "metadata": {
        "id": "2vJUeqkrgVvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b24f2e-8577-44cd-cd81-0eccde7af4b6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from checkpoints/rnnt_epoch_3.pt\n",
            "Loss: 68.52103723716736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install jiwer"
      ],
      "metadata": {
        "id": "Xj1DFE1kL3RZ",
        "outputId": "74325b11-344b-4f6a-a178-40503e4afa59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer"
      ],
      "metadata": {
        "id": "XLpAVrYGL0EQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model, tokenizer, and features\n",
        "model.eval()  # Your RNN-T model  # Shape: [T, D]\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "    for audio in test_audio_files[1000:1010]:\n",
        "        audio_features, _ = preprocess_for_rnnt_torchaudio([os.path.join(audio_directory, audio)])\n",
        "        text_beam = predict_rnnt_beam_search(model, audio_features, tokenizer, \"cuda\")\n",
        "        wer = jiwer.wer(test_texts[i], text_beam)\n",
        "        print(f\"Beam: {text_beam}\")\n",
        "        print(f\"Real text: {test_texts[i]}\")\n",
        "        print(f\"wer: {wer:.4f}\")\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "3WcBnC1yqvIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad57e15-0c9b-44bc-9259-f1fecfc697e4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam: \n",
            "Real text: when were these books written the boy asked\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: the letter said that the payment was delayed\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: fatima is a woman of the desert said the alchemist\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: i want you to help me turn myself into the wind the boy answered\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: not even the tribal chieftains are able to see him when they want to\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: yet it was a little too large for assurance\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: i'm afraid she'd feel ashamed to think she hadn't trusted you\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: i've already had that experience with my sheep and now it's happening with people\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: you had me worried\n",
            "wer: 1.0000\n",
            "Beam: \n",
            "Real text: a whiskey is the perfect end to my day\n",
            "wer: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r checkpoints.zip checkpoints"
      ],
      "metadata": {
        "id": "Qq77BA25Oiko",
        "outputId": "e923edca-8df2-4fed-e7c0-3ffc20923c4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: checkpoints/ (stored 0%)\n",
            "  adding: checkpoints/rnnt_epoch_3.pt (deflated 47%)\n",
            "  adding: checkpoints/rnnt_epoch_2.pt (deflated 47%)\n",
            "  adding: checkpoints/rnnt_epoch_1.pt (deflated 46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoints.zip')"
      ],
      "metadata": {
        "id": "i7MLURLXOmJt",
        "outputId": "d863e496-e020-4e5e-d680-7541265f35e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aa7a467f-ffad-4a3c-8746-6c2d51da4989\", \"checkpoints.zip\", 179932851)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}