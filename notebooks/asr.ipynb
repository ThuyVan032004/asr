{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170bfded-77dd-476e-e6b1-16c606ee0173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle==1.5.12 in /usr/local/lib/python3.11/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (1.17.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2.3.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle==1.5.12) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle==1.5.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle==1.5.12) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "wsv_WL4WdjDB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "11757634-7857-4ef7-b5b5-447137b8d121"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-dev.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pydub"
      ],
      "metadata": {
        "id": "euyV6PTthVWH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[:5]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "eoGLOXhDqYa-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91714962-3baf-4ffb-d45f-0f7e5fe32e5e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = torch.utils.data.random_split(dev_df, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "q3NtLOeJl0Nm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_path, target_sr=16000, max_duration=5):\n",
        "    \"\"\"GPU-accelerated preprocessing pipeline for RNN-T using TorchAudio\"\"\"\n",
        "\n",
        "    # 1. Load audio directly to GPU (PyTorch 2.0+)\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    # 2. Resample if needed\n",
        "    if sr != target_sr:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "    # 3. Trim silence (VAD)\n",
        "    waveform = torchaudio.functional.vad(waveform, sample_rate=target_sr, trigger_level=20)\n",
        "\n",
        "    # 4. Peak normalization (GPU)\n",
        "    waveform = torch.nn.functional.normalize(waveform, dim=-1)\n",
        "\n",
        "    # 5. Fixed-length padding/cropping\n",
        "    max_samples = target_sr * max_duration\n",
        "    if waveform.size(-1) > max_samples:\n",
        "        waveform = waveform[..., :max_samples]\n",
        "    else:\n",
        "        pad_amount = max_samples - waveform.size(-1)\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "    # 6. Extract Log-Mel features (GPU)\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_spec = mel_transform(waveform)\n",
        "    log_mel = torch.log(mel_spec + 1e-6)  # (1, 80, T)\n",
        "\n",
        "    return log_mel.squeeze(0).T  # (T, 80)"
      ],
      "metadata": {
        "id": "Jb9YzYmZKfFU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-dev'\n",
        "audio_files = [train_df.dataset.loc[train_df.indices[i], 'filename'] for i in range(len(train_df))]"
      ],
      "metadata": {
        "id": "0NOcj3K8lzI4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "for audio in audio_files:\n",
        "  audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "  features.append(audio_feature)"
      ],
      "metadata": {
        "id": "ewSOst3wmBmB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [dev_df.loc[idx, 'text'] for idx in train_df.indices]"
      ],
      "metadata": {
        "id": "waufdibonnWz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts[:5]"
      ],
      "metadata": {
        "id": "OMtg8Eb3_EnS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning\n",
        "cleaned_texts = [clean_text(t) for t in texts if isinstance(t, str)]"
      ],
      "metadata": {
        "id": "cfVrlcpd06oC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaned_texts[:5]"
      ],
      "metadata": {
        "id": "syWTuNa25qSn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Simpler pre-tokenizer (preserve apostrophes)\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),\n",
        "    pre_tokenizers.Split(\"'\", behavior=\"isolated\")  # Special handling for apostrophes\n",
        "])\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
        "    vocab_size=3800,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load tokenizer\n",
        "# tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")\n",
        "\n",
        "# # Test encoding\n",
        "# sample_text = \"Dr O'Neill 's patient said We 'll check again tomorrow\"\n",
        "# encoding = tokenizer.encode(sample_text.lower())\n",
        "# print(\"Tokens:\", encoding.tokens)\n",
        "# print(\"IDs:\", encoding.ids)"
      ],
      "metadata": {
        "id": "fcn_7c7v3Xe3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.linear(x)    # (B, T, H)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IR3gNzgsIoEw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, y):\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        return y"
      ],
      "metadata": {
        "id": "V5Ima62TIqXg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "        out = torch.tanh(h_enc + h_dec)  # (B, T, U, H)\n",
        "        return self.linear(out)  # (B, T, U, V)"
      ],
      "metadata": {
        "id": "5K2duKk7IsRI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(encoder_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits.contiguous()"
      ],
      "metadata": {
        "id": "9ZvXpa3aIuE4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue from where the notebook left off\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, texts, tokenizer, max_text_length=100):\n",
        "        self.features = features\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get audio features\n",
        "        audio_feature = self.features[idx]\n",
        "\n",
        "        # Get and tokenize text\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode(text)\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add BOS and EOS tokens\n",
        "        text_ids = [tokenizer.token_to_id(\"<bos>\")] + text_ids + [tokenizer.token_to_id(\"<eos>\")]\n",
        "\n",
        "        # Pad text sequence\n",
        "        if len(text_ids) < self.max_text_length:\n",
        "            pad_amount = self.max_text_length - len(text_ids)\n",
        "            text_ids = text_ids + [tokenizer.token_to_id(\"<pad>\")] * pad_amount\n",
        "        else:\n",
        "            text_ids = text_ids[:self.max_text_length]\n",
        "\n",
        "        return {\n",
        "            'audio': audio_feature,\n",
        "            'text': torch.tensor(text_ids, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "3K1AB6iTIvDv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    features=features,\n",
        "    texts=cleaned_texts,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "rMMOxn6d2v_Q"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cleaned_texts = [clean_text(dev_df.loc[idx, 'text']) for idx in test_df.indices]\n",
        "test_features = []\n",
        "audio_directory = 'cv-valid-dev'\n",
        "audio_files = [test_df.dataset.loc[test_df.indices[i], 'filename'] for i in range(len(test_df))]\n",
        "for audio in audio_files:\n",
        "  audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "  test_features.append(audio_feature)\n",
        "\n",
        "test_dataset = AudioTextDataset(\n",
        "    features=test_features,\n",
        "    texts=test_cleaned_texts,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "yqP8JzSnGA-Q"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HtFUK_q1VQQo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "def collate_fn(batch):\n",
        "    # Pad audio features to same length\n",
        "    audio_features = [item['audio'] for item in batch]\n",
        "    max_audio_len = max(f.size(0) for f in audio_features)\n",
        "\n",
        "    padded_audio = []\n",
        "    for f in audio_features:\n",
        "        pad_amount = max_audio_len - f.size(0)\n",
        "        if pad_amount > 0:\n",
        "            f = torch.nn.functional.pad(f, (0, 0, 0, pad_amount), value=0)\n",
        "        padded_audio.append(f)\n",
        "\n",
        "    audio_tensor = torch.stack(padded_audio)\n",
        "    text_tensor = torch.stack([item['text'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'audio': audio_tensor.to(device),\n",
        "        'text': text_tensor.to(device)\n",
        "    }\n",
        "\n",
        "batch_size = 2\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "83SDmsXp2y_4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnnt_loss(logits, targets, input_lengths, target_lengths, blank=0):\n",
        "    # logits: (B, T, U, V)\n",
        "    # targets: (B, U)\n",
        "    # input_lengths: (B,) - length of each audio sequence\n",
        "    # target_lengths: (B,) - length of each text sequence\n",
        "\n",
        "    # Ensure logits are contiguous\n",
        "    if not logits.is_contiguous():\n",
        "        logits = logits.contiguous()\n",
        "\n",
        "    # Convert to log-probs\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Transpose for torchaudio's expectation (T, B, U, V)\n",
        "    log_probs = log_probs.permute(1, 0, 2, 3)\n",
        "\n",
        "    # Ensure the permuted tensor is contiguous\n",
        "    if not log_probs.is_contiguous():\n",
        "        log_probs = log_probs.contiguous()\n",
        "\n",
        "    # Compute RNN-T loss\n",
        "    loss = torchaudio.functional.rnnt_loss(\n",
        "        log_probs,\n",
        "        targets,\n",
        "        input_lengths,\n",
        "        target_lengths,\n",
        "        blank=blank,\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "LXG052Jp219E"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "def train_epoch(model, dataloader, optimizer, accumulation_steps=4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        audio = batch['audio']\n",
        "        text = batch['text']\n",
        "\n",
        "        # Get input/target lengths\n",
        "        input_lengths = torch.tensor([a.size(0) for a in audio], dtype=torch.int32, device=device)\n",
        "        target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], dtype=torch.int32, device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(audio, text[:, :-1])  # Exclude EOS token for decoder input\n",
        "\n",
        "        # Compute loss\n",
        "        loss = rnnt_loss(\n",
        "            logits.contiguous(),\n",
        "            text[:, 1:].type(torch.int32),  # Exclude BOS token for targets\n",
        "            input_lengths,\n",
        "            target_lengths - 1  # Subtract 1 because we excluded BOS\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "pKD5Qqfu26ur"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            audio = batch['audio']\n",
        "            text = batch['text']\n",
        "\n",
        "            input_lengths = torch.tensor([a.size(0) for a in audio], dtype=torch.int32, device=device)\n",
        "            target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], dtype=torch.int32, device=device)\n",
        "\n",
        "            logits = model(audio, text[:, :-1])\n",
        "\n",
        "            loss = rnnt_loss(\n",
        "                logits.contiguous(),\n",
        "                text[:, 1:],\n",
        "                input_lengths,\n",
        "                target_lengths - 1\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "w80gsKPg3BPZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer)\n",
        "    val_loss = evaluate(model, test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "XOUrFrPr3EUw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6396e898-ab88-4cc9-805f-f13279152c6a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "batch dimension mismatch between logits and logit_lengths",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-4e0d1f22fbc0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-840fe4630007>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, accumulation_steps)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         loss = rnnt_loss(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Exclude BOS token for targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-e81ef3ab0d2b>\u001b[0m in \u001b[0;36mrnnt_loss\u001b[0;34m(logits, targets, input_lengths, target_lengths, blank)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Compute RNN-T loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     loss = torchaudio.functional.rnnt_loss(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mrnnt_loss\u001b[0;34m(logits, targets, logit_lengths, target_lengths, blank, clamp, reduction, fused_log_softmax)\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0mblank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m     costs, _ = torch.ops.torchaudio.rnnt_loss(\n\u001b[0m\u001b[1;32m   1806\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: batch dimension mismatch between logits and logit_lengths"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference function\n",
        "def predict(model, audio_feature, tokenizer, max_decode_len=100):\n",
        "    model.eval()\n",
        "\n",
        "    # Add batch dimension\n",
        "    audio_feature = audio_feature.unsqueeze(0).to(device)\n",
        "    input_length = torch.tensor([audio_feature.size(1)], device=device)\n",
        "\n",
        "    # Initialize with BOS token\n",
        "    decoded = [tokenizer.token_to_id(\"<bos>\")]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h_enc = model.encoder(audio_feature)  # (1, T, H)\n",
        "\n",
        "        for _ in range(max_decode_len):\n",
        "            # Get last predicted token\n",
        "            last_token = torch.tensor([decoded[-1]], device=device).unsqueeze(0)\n",
        "\n",
        "            # Decoder step\n",
        "            h_dec = model.decoder(last_token)  # (1, 1, H)\n",
        "\n",
        "            # Joint network\n",
        "            logits = model.joint(h_enc, h_dec)  # (1, T, 1, V)\n",
        "            log_probs = torch.nn.functional.log_softmax(logits.squeeze(2), dim=-1)  # (1, T, V)\n",
        "\n",
        "            # Sum over time (approximate)\n",
        "            scores = log_probs.sum(1)  # (1, V)\n",
        "            next_token = scores.argmax(-1).item()\n",
        "\n",
        "            # Stop if EOS is predicted\n",
        "            if next_token == tokenizer.token_to_id(\"<eos>\"):\n",
        "                break\n",
        "\n",
        "            decoded.append(next_token)\n",
        "\n",
        "    # Convert to text\n",
        "    tokens = tokenizer.decode(decoded[1:])  # Skip BOS\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "VhFykH0z2rKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inference\n",
        "test_idx = 0\n",
        "test_audio = features[test_idx]\n",
        "test_text = cleaned_texts[test_idx]\n",
        "\n",
        "predicted_text = predict(model, test_audio, tokenizer)\n",
        "print(f\"Original: {test_text}\")\n",
        "print(f\"Predicted: {predicted_text}\")"
      ],
      "metadata": {
        "id": "KXJLSqV53G8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}