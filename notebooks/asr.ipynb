{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "Mg_YAjQ9ttKS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "5bc43126-f400-4c0e-c0fc-08365464d7dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-train.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df[:10000]"
      ],
      "metadata": {
        "id": "SPCU8pwmKP0z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[2000:2050]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b541c9c3-87ac-4b5b-906b-dd0fe5db5b81"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-train'\n",
        "\n",
        "train_size = int(0.8 * len(dev_df))\n",
        "train_df = dev_df.iloc[:train_size]\n",
        "test_df = dev_df.iloc[train_size+1:]\n",
        "\n",
        "train_audio_files = [os.path.join(f) for f in train_df['filename']]\n",
        "train_texts = train_df['text'].tolist()\n",
        "\n",
        "test_audio_files = [os.path.join(f) for f in test_df['filename']]\n",
        "test_texts = test_df['text'].tolist()"
      ],
      "metadata": {
        "id": "mzKLy1FmUBqo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.info()"
      ],
      "metadata": {
        "id": "MbdikRp0M3x6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<blank>\"],\n",
        "    vocab_size=12600,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in train_texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "RQhYjuBSMj0v"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "IwEcJmCK_o1j"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_paths, target_sr=16000, max_duration=2, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing pipeline for RNN-T\n",
        "    Returns:\n",
        "        log_mel: (batch_size, T, 80) tensor of log Mel spectrograms\n",
        "        lengths: (batch_size,) tensor of actual lengths in frames\n",
        "    \"\"\"\n",
        "    # Initialize mel transform once\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        win_length=400,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_specs = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    # Calculate max_len in frames based on max_duration\n",
        "    max_len = int(max_duration * target_sr / 160)  # hop_length is 160\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # 1. Load and resample\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "        waveform = waveform.to(device)\n",
        "\n",
        "        if sr != target_sr:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "        # 2. Normalize\n",
        "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-8)\n",
        "\n",
        "        # 3. Mel transform\n",
        "        mel_spec = mel_transform(waveform)  # (channels, n_mels, time)\n",
        "        log_mel = torch.log(mel_spec + 1e-8)\n",
        "        mean = log_mel.mean(dim=2, keepdim=True)\n",
        "        std = log_mel.std(dim=2, keepdim=True) + 1e-8\n",
        "        log_mel = (log_mel - mean) / std\n",
        "        # Permute to (channels, time, n_mels)\n",
        "        log_mel = log_mel.permute(0, 2, 1)\n",
        "        # 4. Padding/trimming\n",
        "        if log_mel.size(1) > max_len:\n",
        "            log_mel = log_mel[:, :max_len+1, :]\n",
        "        else:\n",
        "            pad_amount = max_len + 1 - log_mel.size(1)\n",
        "            log_mel = torch.nn.functional.pad(log_mel, (0, 0, 0, pad_amount))\n",
        "\n",
        "        mel_specs.append(log_mel.squeeze(0))  # Remove channel dimension if single channel\n",
        "        audio_lengths.append(log_mel.size(1))\n",
        "\n",
        "    # Stack into batch (batch_size, T, n_mels)\n",
        "    mel_specs = torch.stack(mel_specs)\n",
        "    audio_lengths = torch.tensor(audio_lengths, dtype=torch.int32)\n",
        "\n",
        "    return mel_specs, audio_lengths"
      ],
      "metadata": {
        "id": "MTcU-suGxili"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, input_dim)\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)   # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # y: (B, U)\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        y = self.dropout(y)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        # Use a more robust approach with a multi-layer network\n",
        "        self.joint = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "\n",
        "        # Get batch size and sequence lengths\n",
        "        batch_size = h_enc.size(0)\n",
        "        T = h_enc.size(1)  # Audio sequence length\n",
        "        U = h_dec.size(1)  # Text sequence length\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "\n",
        "        # Expand to create the full cartesian product for alignment\n",
        "        h_enc = h_enc.expand(-1, -1, U, -1)  # (B, T, U, H)\n",
        "        h_dec = h_dec.expand(-1, T, -1, -1)  # (B, T, U, H)\n",
        "\n",
        "        # Concatenate features\n",
        "        joint = torch.cat([h_enc, h_dec], dim=-1)  # (B, T, U, 2H)\n",
        "\n",
        "        # Apply joint network to get logits\n",
        "        logits = self.joint(joint)  # (B, T, U, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(decoder_dim, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: (B, T, features), y: (B, U)\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "\n",
        "        # Both should now have the same hidden dimension\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_files, texts, tokenizer, max_text_length=100,\n",
        "                 target_sr=16000, max_duration=2):\n",
        "        \"\"\"\n",
        "        Dataset class that handles:\n",
        "        - File paths for batch audio processing\n",
        "        - Text sequences with tokenization\n",
        "        - Automatic audio preprocessing in batches\n",
        "        \"\"\"\n",
        "        self.audio_files = [str(f) for f in audio_files]  # Ensure string paths\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.target_sr = target_sr\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns single item with:\n",
        "        - audio_file: Path for batch processing\n",
        "        - text: Raw text string for tokenization in collate\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'audio_file': self.audio_files[idx],\n",
        "            'text': self.texts[idx],\n",
        "            'tokenizer': self.tokenizer,  # Pass tokenizer for collate_fn\n",
        "            'max_text_length': self.max_text_length\n",
        "        }\n",
        "\n",
        "def collate_fn(batch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Custom collate function that:\n",
        "    1. Processes audio files in batch using preprocess_for_rnnt_torchaudio\n",
        "    2. Tokenizes and pads text sequences\n",
        "    3. Handles audio length preservation\n",
        "    \"\"\"\n",
        "    # Extract batch components\n",
        "    audio_files = [item['audio_file'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Batch process audio files\n",
        "    audio_features, audio_lengths = preprocess_for_rnnt_torchaudio(\n",
        "        audio_files,\n",
        "        target_sr=batch[0].get('target_sr', 16000),\n",
        "        max_duration=batch[0].get('max_duration', 2),\n",
        "        device=device\n",
        "    )  # (B, T, 80)\n",
        "\n",
        "    target_lengths = [item['max_text_length'] - 2 for item in batch]\n",
        "\n",
        "    # Tokenize and pad texts\n",
        "    text_sequences = []\n",
        "    for item in batch:\n",
        "        encoding = item['tokenizer'].encode(item['text'])\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add special tokens\n",
        "        text_ids = [\n",
        "            item['tokenizer'].token_to_id(\"<bos>\")\n",
        "        ] + text_ids + [\n",
        "            item['tokenizer'].token_to_id(\"<eos>\")\n",
        "        ]\n",
        "\n",
        "        # Pad or truncate\n",
        "        max_len = item['max_text_length']\n",
        "        if len(text_ids) < max_len:\n",
        "            text_ids = text_ids + [item['tokenizer'].token_to_id(\"<pad>\")] * (max_len - len(text_ids))\n",
        "        else:\n",
        "            text_ids = text_ids[:max_len]\n",
        "\n",
        "        text_sequences.append(torch.tensor(text_ids, dtype=torch.int32))\n",
        "\n",
        "    target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.int32)\n",
        "    # Stack text sequences\n",
        "    text_tensor = torch.stack(text_sequences)\n",
        "\n",
        "    return {\n",
        "        'audio': audio_features.to(device),  # (B, T, 80)\n",
        "        'text': text_tensor.to(device),     # (B, U_max)\n",
        "        'target_lengths': target_lengths_tensor.to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, device=device) # (B,)\n",
        "    }"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision=True):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    # Enable mixed precision for faster training and lower memory usage\n",
        "    scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get batch data\n",
        "        audio = batch['audio']  # (B, T, 80)\n",
        "        text = batch['text']    # (B, U)\n",
        "\n",
        "        # Get lengths\n",
        "        input_lengths = batch['audio_lengths']\n",
        "        target_lengths = batch['target_lengths']\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mixed_precision:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Forward pass - encoder\n",
        "                encoder_output = model.encoder(audio)\n",
        "\n",
        "                # Forward pass - decoder and joint\n",
        "                h_dec = model.decoder(text[:, :-1])  # (B, U-1, H)\n",
        "                logits = model.joint(encoder_output, h_dec)  # (B, T, U-1, V)\n",
        "\n",
        "                # For targets, use labels shifted right (exclude BOS)\n",
        "                targets = text[:, 1:-1].contiguous()  # Remove only BOS for targets\n",
        "\n",
        "                # Compute loss\n",
        "                blank_id = tokenizer.token_to_id('<blank>')\n",
        "                loss = torchaudio.functional.rnnt_loss(\n",
        "                    logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                    targets=targets,\n",
        "                    logit_lengths=input_lengths,\n",
        "                    target_lengths=target_lengths,\n",
        "                    blank=blank_id,\n",
        "                    reduction='mean'\n",
        "                )\n",
        "\n",
        "            # Backward pass with scaled gradients\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Lower clip threshold\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Standard training without mixed precision\n",
        "            encoder_output = model.encoder(audio)\n",
        "            h_dec = model.decoder(text[:, :-1])\n",
        "            logits = model.joint(encoder_output, h_dec)\n",
        "\n",
        "            targets = text[:, 1:-1].contiguous()\n",
        "\n",
        "            blank_id = tokenizer.token_to_id('<blank>')\n",
        "            loss = torchaudio.functional.rnnt_loss(\n",
        "                logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                targets=targets,\n",
        "                logit_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=blank_id,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update learning rate with scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    return total_loss / steps"
      ],
      "metadata": {
        "id": "AbKbvmzwtkc3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "IIa9x7nMhF5c"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in train_audio_files],\n",
        "    texts=train_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in test_audio_files],\n",
        "    texts=test_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        ")"
      ],
      "metadata": {
        "id": "Lhzo3JhgKWxZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5,  # L2 regularization\n",
        "        betas=(0.9, 0.999))\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=5e-4,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=100,\n",
        "        pct_start=0.2,  # Warm-up for first 10% of training\n",
        "        div_factor=10,  # Initial LR = max_lr/div_factor\n",
        "        final_div_factor=100,  # Final LR = initial_lr/final_div_factor\n",
        "    )\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    # Train for one epoch\n",
        "    train_loss = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        mixed_precision=True\n",
        "    )\n",
        "\n",
        "    print(f\"Train Loss {epoch}: {train_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if save_dir: # Use save_dir instead of args.save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(save_dir, f\"rnnt_epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020",
        "collapsed": true,
        "outputId": "c23ba625-cd18-4e6c-e09b-b9c4a4e26e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss 0: 154.2453\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_1.pt\n",
            "Train Loss 1: 41.3094\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_2.pt\n",
            "Train Loss 2: 40.3018\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_3.pt\n",
            "Train Loss 3: 39.5105\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_4.pt\n",
            "Train Loss 4: 38.6023\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_5.pt\n",
            "Train Loss 5: 37.7157\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_6.pt\n",
            "Train Loss 6: 37.0213\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_7.pt\n",
            "Train Loss 7: 36.2439\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_8.pt\n",
            "Train Loss 8: 35.2095\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_9.pt\n",
            "Train Loss 9: 34.3795\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_10.pt\n",
            "Train Loss 10: 33.8540\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_11.pt\n",
            "Train Loss 11: 32.9307\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_12.pt\n",
            "Train Loss 12: 32.2270\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_13.pt\n",
            "Train Loss 13: 29.9436\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_14.pt\n",
            "Train Loss 14: 28.4698\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_15.pt\n",
            "Train Loss 15: 26.8136\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_16.pt\n",
            "Train Loss 16: 25.0882\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_17.pt\n",
            "Train Loss 17: 23.6219\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_18.pt\n",
            "Train Loss 18: 22.0702\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_19.pt\n",
            "Train Loss 19: 20.6959\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_20.pt\n",
            "Train Loss 20: 19.7556\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_21.pt\n",
            "Train Loss 21: 18.7902\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_22.pt\n",
            "Train Loss 22: 17.2051\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_23.pt\n",
            "Train Loss 23: 16.4776\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_24.pt\n",
            "Train Loss 24: 15.5960\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_25.pt\n",
            "Train Loss 25: 15.0015\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_26.pt\n",
            "Train Loss 26: 14.4354\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_27.pt\n",
            "Train Loss 27: 13.8185\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_28.pt\n",
            "Train Loss 28: 13.5120\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_29.pt\n",
            "Train Loss 29: 12.7387\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_30.pt\n",
            "Train Loss 30: 12.4237\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_31.pt\n",
            "Train Loss 31: 12.0470\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_32.pt\n",
            "Train Loss 32: 11.8018\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_33.pt\n",
            "Train Loss 33: 11.5771\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_34.pt\n",
            "Train Loss 34: 11.2946\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_35.pt\n",
            "Train Loss 35: 11.2019\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_36.pt\n",
            "Train Loss 36: 10.8926\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_37.pt\n",
            "Train Loss 37: 10.8923\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_38.pt\n",
            "Train Loss 38: 10.7366\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_39.pt\n",
            "Train Loss 39: 10.4828\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_40.pt\n",
            "Train Loss 40: 10.3334\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_41.pt\n",
            "Train Loss 41: 10.0733\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_42.pt\n",
            "Train Loss 42: 9.8861\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_43.pt\n",
            "Train Loss 43: 9.5783\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_44.pt\n",
            "Train Loss 44: 9.3789\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_45.pt\n",
            "Train Loss 45: 9.1963\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_46.pt\n",
            "Train Loss 46: 9.0433\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_47.pt\n",
            "Train Loss 47: 8.9313\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_48.pt\n",
            "Train Loss 48: 8.7185\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_49.pt\n",
            "Train Loss 49: 8.5782\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_50.pt\n",
            "Train Loss 50: 8.3120\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_51.pt\n",
            "Train Loss 51: 8.1219\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_52.pt\n",
            "Train Loss 52: 7.9851\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_53.pt\n",
            "Train Loss 53: 7.8149\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_54.pt\n",
            "Train Loss 54: 7.6799\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_55.pt\n",
            "Train Loss 55: 7.5162\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_56.pt\n",
            "Train Loss 56: 7.3722\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_57.pt\n",
            "Train Loss 57: 7.1732\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_58.pt\n",
            "Train Loss 58: 7.0455\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_59.pt\n",
            "Train Loss 59: 6.8859\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_60.pt\n",
            "Train Loss 60: 6.7135\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_61.pt\n",
            "Train Loss 61: 6.4982\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_62.pt\n",
            "Train Loss 62: 6.3418\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_63.pt\n",
            "Train Loss 63: 6.1855\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_64.pt\n",
            "Train Loss 64: 6.0655\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_65.pt\n",
            "Train Loss 65: 5.8538\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_66.pt\n",
            "Train Loss 66: 5.7106\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_67.pt\n",
            "Train Loss 67: 5.5353\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_68.pt\n",
            "Train Loss 68: 5.3862\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_69.pt\n",
            "Train Loss 69: 5.2220\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_70.pt\n",
            "Train Loss 70: 5.0854\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_71.pt\n",
            "Train Loss 71: 4.9775\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_72.pt\n",
            "Train Loss 72: 4.7915\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_73.pt\n",
            "Train Loss 73: 4.6933\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_74.pt\n",
            "Train Loss 74: 4.5539\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_75.pt\n",
            "Train Loss 75: 4.4479\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_76.pt\n",
            "Train Loss 76: 4.3039\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_77.pt\n",
            "Train Loss 77: 4.1977\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_78.pt\n",
            "Train Loss 78: 4.0785\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_79.pt\n",
            "Train Loss 79: 3.9818\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_80.pt\n",
            "Train Loss 80: 3.8918\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_81.pt\n",
            "Train Loss 81: 3.8065\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_82.pt\n",
            "Train Loss 82: 3.7287\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_83.pt\n",
            "Train Loss 83: 3.6532\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_84.pt\n",
            "Train Loss 84: 3.5734\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_85.pt\n",
            "Train Loss 85: 3.5213\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_86.pt\n",
            "Train Loss 86: 3.4573\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_87.pt\n",
            "Train Loss 87: 3.4119\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_88.pt\n",
            "Train Loss 88: 3.3601\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_89.pt\n",
            "Train Loss 89: 3.3293\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_90.pt\n",
            "Train Loss 90: 3.2704\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_91.pt\n",
            "Train Loss 91: 3.2616\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_92.pt\n",
            "Train Loss 92: 3.2213\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_93.pt\n",
            "Train Loss 93: 3.2071\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_94.pt\n",
            "Train Loss 94: 3.1879\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_95.pt\n",
            "Train Loss 95: 3.1789\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_96.pt\n",
            "Train Loss 96: 3.1690\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_97.pt\n",
            "Train Loss 97: 3.1584\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_98.pt\n",
            "Train Loss 98: 3.1518\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_99.pt\n",
            "Train Loss 99: 3.1548\n",
            "Saved checkpoint to checkpoints/rnnt_epoch_100.pt\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union"
      ],
      "metadata": {
        "id": "r-8zH-OXGqFI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rnnt_beam_search(\n",
        "    model: torch.nn.Module,\n",
        "    audio_features: torch.Tensor,\n",
        "    tokenizer: object,\n",
        "    device: Union[str, torch.device],\n",
        "    beam_width: int = 5,\n",
        "    max_decode_len: int = 100,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Performs beam search decoding for an RNN-T model.\n",
        "\n",
        "    Args:\n",
        "        model: The RNN-T model\n",
        "        audio_features: The audio features tensor\n",
        "        tokenizer: The tokenizer object\n",
        "        device: The device to run inference on\n",
        "        beam_width: Width of the beam search\n",
        "        max_decode_len: Maximum decoding length\n",
        "\n",
        "    Returns:\n",
        "        The best decoded transcript\n",
        "    \"\"\"\n",
        "    # Add batch dimension if missing (shape: [1, T, D])\n",
        "    if len(audio_features.shape) == 2:\n",
        "        audio_features = audio_features.unsqueeze(0).to(device)\n",
        "    else:\n",
        "        audio_features = audio_features.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode the audio features\n",
        "        h_enc = model.encoder(audio_features)  # [1, T, H]\n",
        "\n",
        "        # Initialize the beam\n",
        "        # Each beam element: (negative log prob, sequence, decoder state)\n",
        "        bos_token = tokenizer.token_to_id(\"<bos>\")\n",
        "        eos_token = tokenizer.token_to_id(\"<eos>\")\n",
        "        blank_token = tokenizer.token_to_id(\"<blank>\")\n",
        "\n",
        "        # Initialize beam with just the beginning token\n",
        "        current_idx = torch.tensor([[bos_token]], device=device)\n",
        "        initial_decoder_output = model.decoder(current_idx)  # [1, 1, H]\n",
        "\n",
        "        # Start with a single beam\n",
        "        beams = [(0.0, [bos_token], initial_decoder_output)]\n",
        "        finished_beams = []\n",
        "\n",
        "        # Main beam search loop\n",
        "        for step in range(max_decode_len):\n",
        "            candidates = []\n",
        "\n",
        "            # Expand each current beam\n",
        "            for log_prob, sequence, decoder_output in beams:\n",
        "                # Skip if sequence already ended\n",
        "                if sequence[-1] == eos_token:\n",
        "                    finished_beams.append((log_prob, sequence))\n",
        "                    continue\n",
        "\n",
        "                # Run joint network on the encoded audio and last decoder output\n",
        "                logits = model.joint(h_enc, decoder_output)  # [1, T, 1, V]\n",
        "\n",
        "                # Sum over time dimension for alignment scores\n",
        "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "                scores = log_probs.sum(dim=1).squeeze(1)  # [1, V]\n",
        "\n",
        "                # Get top-k tokens\n",
        "                top_log_probs, top_indices = scores.topk(beam_width)\n",
        "\n",
        "                # Add candidates to our list\n",
        "                for i in range(beam_width):\n",
        "                    token_id = top_indices[0, i].item()\n",
        "                    token_log_prob = top_log_probs[0, i].item()\n",
        "\n",
        "                    # Skip blank tokens in final output but consider them for alignment\n",
        "                    if token_id == blank_token:\n",
        "                        candidates.append((log_prob + token_log_prob, sequence.copy(), decoder_output))\n",
        "                    else:\n",
        "                        new_sequence = sequence.copy() + [token_id]\n",
        "\n",
        "                        # Update decoder state with new token\n",
        "                        new_token = torch.tensor([[token_id]], device=device)\n",
        "                        new_decoder_output = model.decoder(new_token)\n",
        "\n",
        "                        candidates.append((log_prob + token_log_prob, new_sequence, new_decoder_output))\n",
        "\n",
        "            # Keep only the best beam_width candidates\n",
        "            candidates.sort(key=lambda x: x[0], reverse=True)  # Sort by log probability (higher is better)\n",
        "            beams = candidates[:beam_width]\n",
        "\n",
        "            # Early stopping if all beams have finished\n",
        "            if all(beam[1][-1] == eos_token for beam in beams):\n",
        "                break\n",
        "\n",
        "        # Add any unfinished beams to finished_beams\n",
        "        for log_prob, sequence, _ in beams:\n",
        "            if sequence[-1] != eos_token:\n",
        "                finished_beams.append((log_prob, sequence))\n",
        "\n",
        "        best_sequence = None\n",
        "        # print(\"Number of beams: \", finished_beams)\n",
        "        # Select the most likely beam\n",
        "        if finished_beams:\n",
        "            best_beam = max(finished_beams, key=lambda x: x[0])\n",
        "            best_sequence = best_beam[1]\n",
        "        else:\n",
        "            best_sequence = beams[0][1]  # Best unfinished beam\n",
        "\n",
        "        # Remove special tokens (BOS, EOS) for the final output\n",
        "        filtered_sequence = [token for token in best_sequence\n",
        "                            if token != bos_token and token != eos_token]\n",
        "\n",
        "        return tokenizer.decode(filtered_sequence)"
      ],
      "metadata": {
        "id": "t7iEpIvbLUOr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Specify the directory where checkpoints are saved\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Get the last checkpoint file\n",
        "last_checkpoint_file = 'rnnt_epoch_100.pt'\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint_path = os.path.join(save_dir, last_checkpoint_file)\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Create a new instance of your model (if needed)\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "\n",
        "# Load the model state dictionary from the checkpoint\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Load the optimizer state dictionary (optional)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "\n",
        "print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "print(f\"Loss: {checkpoint['loss']}\")"
      ],
      "metadata": {
        "id": "2vJUeqkrgVvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6380e4-2eda-4a60-f523-3781d160ddcc"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from checkpoints/rnnt_epoch_100.pt\n",
            "Loss: 3.1547877920866014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install jiwer"
      ],
      "metadata": {
        "id": "Xj1DFE1kL3RZ",
        "outputId": "fde360ae-fe63-4048-f342-b6bdbb658ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer"
      ],
      "metadata": {
        "id": "XLpAVrYGL0EQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model, tokenizer, and features\n",
        "model.eval()  # Your RNN-T model  # Shape: [T, D]\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "    for audio in test_audio_files[1000:1010]:\n",
        "        audio_features, _ = preprocess_for_rnnt_torchaudio([os.path.join(audio_directory, audio)])\n",
        "        text_beam = predict_rnnt_beam_search(model, audio_features, tokenizer, \"cuda\")\n",
        "        wer = jiwer.wer(test_texts[i], text_beam)\n",
        "        print(f\"Beam: {text_beam}\")\n",
        "        print(f\"Real text: {test_texts[i]}\")\n",
        "        print(f\"wer: {wer:.4f}\")\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "3WcBnC1yqvIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a354ec1-4725-4331-a904-863bbb1ea0f0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: when were these books written the boy asked\n",
            "wer: 1.0000\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: the letter said that the payment was delayed\n",
            "wer: 0.8750\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: fatima is a woman of the desert said the alchemist\n",
            "wer: 1.0000\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: i want you to help me turn myself into the wind the boy answered\n",
            "wer: 1.0000\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: not even the tribal chieftains are able to see him when they want to\n",
            "wer: 0.8571\n",
            "Beam: no doubt the impact had caused a flash of fire\n",
            "Real text: yet it was a little too large for assurance\n",
            "wer: 1.1111\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: i'm afraid she'd feel ashamed to think she hadn't trusted you\n",
            "wer: 0.9091\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: i've already had that experience with my sheep and now it's happening with people\n",
            "wer: 0.9286\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: you had me worried\n",
            "wer: 1.5000\n",
            "Beam: the top had certainly ceased to rotate\n",
            "Real text: a whiskey is the perfect end to my day\n",
            "wer: 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r checkpoints.zip checkpoints"
      ],
      "metadata": {
        "id": "Qq77BA25Oiko",
        "outputId": "f220c731-f438-47af-ff83-d56f88e0cbb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: checkpoints\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r checkpoints.zip . -i checkpoints)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoints.zip')"
      ],
      "metadata": {
        "id": "i7MLURLXOmJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}