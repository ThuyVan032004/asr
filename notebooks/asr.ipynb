{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "wsv_WL4WdjDB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "ae06130e-9e7f-4bb8-9e38-7595bbcdb47c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-dev.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pydub"
      ],
      "metadata": {
        "id": "euyV6PTthVWH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[:5]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8cd2b1-6ecc-4ab7-e08d-637691f0704d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = torch.utils.data.random_split(dev_df, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "q3NtLOeJl0Nm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_path, target_sr=16000, max_duration=3):\n",
        "    \"\"\"GPU-accelerated preprocessing pipeline for RNN-T using TorchAudio\"\"\"\n",
        "\n",
        "    # 1. Load audio directly to GPU (PyTorch 2.0+)\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    # 2. Resample if needed\n",
        "    if sr != target_sr:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "    # 3. Trim silence (VAD)\n",
        "    waveform = torchaudio.functional.vad(waveform, sample_rate=target_sr, trigger_level=20)\n",
        "\n",
        "    # 4. Peak normalization (GPU)\n",
        "    waveform = torch.nn.functional.normalize(waveform, dim=-1)\n",
        "\n",
        "    # 5. Fixed-length padding/cropping\n",
        "    max_samples = target_sr * max_duration\n",
        "    if waveform.size(-1) > max_samples:\n",
        "        waveform = waveform[..., :max_samples]\n",
        "    else:\n",
        "        pad_amount = max_samples - waveform.size(-1)\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "    # 6. Extract Log-Mel features (GPU)\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_spec = mel_transform(waveform)\n",
        "    log_mel = torch.log(mel_spec + 1e-6)  # (1, 80, T)\n",
        "\n",
        "    return log_mel.squeeze(0).T  # (T, 80)"
      ],
      "metadata": {
        "id": "Jb9YzYmZKfFU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-dev'\n",
        "audio_files = [train_df.dataset.loc[train_df.indices[i], 'filename'] for i in range(len(train_df))]"
      ],
      "metadata": {
        "id": "0NOcj3K8lzI4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "for audio in audio_files:\n",
        "  audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "  features.append(audio_feature)"
      ],
      "metadata": {
        "id": "ewSOst3wmBmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [dev_df.loc[idx, 'text'] for idx in train_df.indices]"
      ],
      "metadata": {
        "id": "waufdibonnWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts[:5]"
      ],
      "metadata": {
        "id": "OMtg8Eb3_EnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning\n",
        "cleaned_texts = [clean_text(t) for t in texts if isinstance(t, str)]"
      ],
      "metadata": {
        "id": "cfVrlcpd06oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaned_texts[:5]"
      ],
      "metadata": {
        "id": "syWTuNa25qSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
        "    vocab_size=6150,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load tokenizer\n",
        "# tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")\n",
        "\n",
        "# # Test encoding\n",
        "# sample_text = \"hello mr. sunshine!\"\n",
        "# cleaned_sample = clean_text(sample_text)\n",
        "# print(\"Cleaned sample: \", cleaned_sample)\n",
        "# encoding = tokenizer.encode(sample_text.lower())\n",
        "# print(\"Tokens:\", encoding.tokens)\n",
        "# print(\"Length: \", len(encoding.tokens))\n",
        "# print(\"IDs:\", encoding.ids)\n",
        "# print(\"Length: \", len(encoding.ids))"
      ],
      "metadata": {
        "id": "fcn_7c7v3Xe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"vocal size: \", tokenizer.get_vocab_size())"
      ],
      "metadata": {
        "id": "5soCVvhQadZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.linear(x)    # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, y):\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "        out = torch.tanh(h_enc + h_dec)  # (B, T, U, H)\n",
        "        return self.linear(out)  # (B, T, U, V)\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(encoder_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits.contiguous()\n",
        "\n",
        "# Continue from where the notebook left off\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, texts, tokenizer, max_text_length=200):\n",
        "        self.features = features\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get audio features\n",
        "        audio_feature = self.features[idx]\n",
        "\n",
        "        # Get and tokenize text\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode(text)\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add BOS and EOS tokens\n",
        "        text_ids = [tokenizer.token_to_id(\"<bos>\")] + text_ids + [tokenizer.token_to_id(\"<eos>\")]\n",
        "\n",
        "        # Pad text sequence\n",
        "        if len(text_ids) < self.max_text_length:\n",
        "            pad_amount = self.max_text_length - len(text_ids)\n",
        "            text_ids = text_ids + [tokenizer.token_to_id(\"<pad>\")] * pad_amount\n",
        "        else:\n",
        "            text_ids = text_ids[:self.max_text_length]\n",
        "\n",
        "        return {\n",
        "            'audio': audio_feature,\n",
        "            'text': torch.tensor(text_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Pad audio features to same length\n",
        "    audio_features = [item['audio'] for item in batch]\n",
        "    max_audio_len = max(f.size(0) for f in audio_features)\n",
        "\n",
        "    padded_audio = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    for f in audio_features:\n",
        "        audio_lengths.append(f.size(0))  # Original length before padding\n",
        "        pad_amount = max_audio_len - f.size(0)\n",
        "        if pad_amount > 0:\n",
        "            f = torch.nn.functional.pad(f, (0, 0, 0, pad_amount), value=0)\n",
        "        padded_audio.append(f)\n",
        "\n",
        "    audio_tensor = torch.stack(padded_audio)\n",
        "    text_tensor = torch.stack([item['text'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'audio': audio_tensor.to(device),\n",
        "        'text': text_tensor.to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, dtype=torch.int32, device=device)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    features=features,\n",
        "    texts=cleaned_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cleaned_texts = [clean_text(dev_df.loc[idx, 'text']) for idx in test_df.indices]\n",
        "test_features = []\n",
        "audio_directory = 'cv-valid-dev'\n",
        "audio_files = [test_df.dataset.loc[test_df.indices[i], 'filename'] for i in range(len(test_df))]\n",
        "for audio in audio_files:\n",
        "  audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "  test_features.append(audio_feature)\n",
        "\n",
        "test_dataset = AudioTextDataset(\n",
        "    features=test_features,\n",
        "    texts=test_cleaned_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnnt_loss(logits, targets, input_lengths, target_lengths, blank=0):\n",
        "    # logits: (B, T, U, V)\n",
        "    # targets: (B, U)\n",
        "    # input_lengths: (B,) - length of each audio sequence\n",
        "    # target_lengths: (B,) - length of each text sequence\n",
        "\n",
        "    # Ensure logits are contiguous\n",
        "    logits = logits.contiguous()\n",
        "\n",
        "    # Convert to log-probs\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Permute to (T, B, U, V) for torchaudio\n",
        "    log_probs = log_probs.permute(1, 0, 2, 3)\n",
        "    log_probs = log_probs.contiguous()\n",
        "\n",
        "    # Ensure targets are contiguous and correct dtype\n",
        "    targets = targets.contiguous().to(torch.int32)\n",
        "\n",
        "    # Compute RNN-T loss\n",
        "    loss = torchaudio.functional.rnnt_loss(\n",
        "        log_probs,\n",
        "        targets,\n",
        "        input_lengths,\n",
        "        target_lengths,\n",
        "        blank=blank,\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        audio = batch['audio']\n",
        "        text = batch['text']\n",
        "\n",
        "        # Fix 1: Use the audio_lengths provided by collate_fn\n",
        "        input_lengths = batch['audio_lengths']\n",
        "\n",
        "        # Fix 2: Adjust target_lengths to exclude EOS token\n",
        "        target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], dtype=torch.int32, device=device)\n",
        "        target_lengths = target_lengths - 1  # Subtract 1 for EOS token\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(audio, text[:, :-1]) # Pass unshifted sequence as target\n",
        "\n",
        "        # Transpose the logits to match the expected shape (T, B, U, V)\n",
        "        logits = logits.permute(1, 0, 2, 3)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = rnnt_loss(\n",
        "            logits.contiguous(),\n",
        "            text[:, 1:].contiguous(), # targets (shifted by 1 to the right)\n",
        "            input_lengths,\n",
        "            target_lengths  # Subtract 1 for EOS token\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            audio = batch['audio']\n",
        "            text = batch['text']\n",
        "\n",
        "            # Fix 1: Use the audio_lengths provided by collate_fn\n",
        "            input_lengths = batch['audio_lengths']\n",
        "\n",
        "            # Fix 2: Adjust target_lengths to exclude EOS token\n",
        "            target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], dtype=torch.int32, device=device)\n",
        "            target_lengths = target_lengths - 1  # Subtract 1 for EOS token\n",
        "\n",
        "            logits = model(audio, text[:, :-1]) # Pass unshifted sequence as target\n",
        "\n",
        "            # Transpose the logits to match the expected shape (T, B, U, V)\n",
        "            logits = logits.permute(1, 0, 2, 3)\n",
        "\n",
        "            loss = rnnt_loss(\n",
        "                logits.contiguous(),\n",
        "                text[:, 1:].contiguous(),\n",
        "                input_lengths,\n",
        "                target_lengths # Subtract 1 for EOS in targets\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "7YhcqMHqEixr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer)\n",
        "    val_loss = evaluate(model, test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Encoder(torch.nn.Module):\n",
        "#     def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "#         super().__init__()\n",
        "#         self.lstm = torch.nn.LSTM(\n",
        "#             input_size=input_dim,\n",
        "#             hidden_size=hidden_dim,\n",
        "#             num_layers=num_layers,\n",
        "#             bidirectional=True,\n",
        "#             batch_first=True,\n",
        "#         )\n",
        "#         self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "#         x = self.linear(x)    # (B, T, H)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "IR3gNzgsIoEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Decoder(torch.nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "#         super().__init__()\n",
        "#         self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "#         self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "#     def forward(self, y):\n",
        "#         y = self.embed(y)     # (B, U, E)\n",
        "#         y, _ = self.lstm(y)   # (B, U, H)\n",
        "#         return y"
      ],
      "metadata": {
        "id": "V5Ima62TIqXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class JointNetwork(torch.nn.Module):\n",
        "#     def __init__(self, hidden_dim, vocab_size):\n",
        "#         super().__init__()\n",
        "#         self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, h_enc, h_dec):\n",
        "#         # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "#         h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "#         h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "#         out = torch.tanh(h_enc + h_dec)  # (B, T, U, H)\n",
        "#         return self.linear(out)  # (B, T, U, V)"
      ],
      "metadata": {
        "id": "5K2duKk7IsRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class RNNTransducer(torch.nn.Module):\n",
        "#     def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "#         super().__init__()\n",
        "#         self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "#         self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "#         self.joint = JointNetwork(encoder_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         h_enc = self.encoder(x)  # (B, T, H)\n",
        "#         h_dec = self.decoder(y)  # (B, U, H)\n",
        "#         logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "#         return logits.contiguous()"
      ],
      "metadata": {
        "id": "9ZvXpa3aIuE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Continue from where the notebook left off\n",
        "\n",
        "# # First, let's prepare the dataset class\n",
        "# class AudioTextDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, features, texts, tokenizer, max_text_length=200):\n",
        "#         self.features = features\n",
        "#         self.texts = texts\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_text_length = max_text_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.features)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Get audio features\n",
        "#         audio_feature = self.features[idx]\n",
        "\n",
        "#         # Get and tokenize text\n",
        "#         text = self.texts[idx]\n",
        "#         encoding = self.tokenizer.encode(text)\n",
        "#         text_ids = encoding.ids\n",
        "\n",
        "#         # Add BOS and EOS tokens\n",
        "#         text_ids = [tokenizer.token_to_id(\"<bos>\")] + text_ids + [tokenizer.token_to_id(\"<eos>\")]\n",
        "\n",
        "#         # Pad text sequence\n",
        "#         if len(text_ids) < self.max_text_length:\n",
        "#             pad_amount = self.max_text_length - len(text_ids)\n",
        "#             text_ids = text_ids + [tokenizer.token_to_id(\"<pad>\")] * pad_amount\n",
        "#         else:\n",
        "#             text_ids = text_ids[:self.max_text_length]\n",
        "\n",
        "#         return {\n",
        "#             'audio': audio_feature,\n",
        "#             'text': torch.tensor(text_ids, dtype=torch.long)\n",
        "#         }"
      ],
      "metadata": {
        "id": "3K1AB6iTIvDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create datasets\n",
        "# train_dataset = AudioTextDataset(\n",
        "#     features=features,\n",
        "#     texts=cleaned_texts,\n",
        "#     tokenizer=tokenizer,\n",
        "# )"
      ],
      "metadata": {
        "id": "rMMOxn6d2v_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_cleaned_texts = [clean_text(dev_df.loc[idx, 'text']) for idx in test_df.indices]\n",
        "# test_features = []\n",
        "# audio_directory = 'cv-valid-dev'\n",
        "# audio_files = [test_df.dataset.loc[test_df.indices[i], 'filename'] for i in range(len(test_df))]\n",
        "# for audio in audio_files:\n",
        "#   audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "#   test_features.append(audio_feature)\n",
        "\n",
        "# test_dataset = AudioTextDataset(\n",
        "#     features=test_features,\n",
        "#     texts=test_cleaned_texts,\n",
        "#     tokenizer=tokenizer,\n",
        "# )"
      ],
      "metadata": {
        "id": "yqP8JzSnGA-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HtFUK_q1VQQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def collate_fn(batch):\n",
        "#     # Pad audio features to same length\n",
        "#     audio_features = [item['audio'] for item in batch]\n",
        "#     max_audio_len = max(f.size(0) for f in audio_features)\n",
        "\n",
        "#     padded_audio = []\n",
        "#     audio_lengths = []\n",
        "\n",
        "#     for f in audio_features:\n",
        "#         audio_lengths.append(f.size(0))  # Original length before padding\n",
        "#         pad_amount = max_audio_len - f.size(0)\n",
        "#         if pad_amount > 0:\n",
        "#             f = torch.nn.functional.pad(f, (0, 0, 0, pad_amount), value=0)\n",
        "#         padded_audio.append(f)\n",
        "\n",
        "#     audio_tensor = torch.stack(padded_audio)\n",
        "#     text_tensor = torch.stack([item['text'] for item in batch])\n",
        "\n",
        "#     return {\n",
        "#         'audio': audio_tensor.to(device),\n",
        "#         'text': text_tensor.to(device),\n",
        "#         'audio_lengths': torch.tensor(audio_lengths, dtype=torch.int32, device=device)\n",
        "#     }"
      ],
      "metadata": {
        "id": "83SDmsXp2y_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def rnnt_loss(logits, targets, input_lengths, target_lengths, blank=0):\n",
        "#     # logits: (B, T, U, V)\n",
        "#     # targets: (B, U)\n",
        "#     # input_lengths: (B,) - length of each audio sequence\n",
        "#     # target_lengths: (B,) - length of each text sequence\n",
        "\n",
        "#     # Ensure logits are contiguous\n",
        "#     logits = logits.contiguous()\n",
        "\n",
        "#     # Convert to log-probs\n",
        "#     log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "#     # Permute to (T, B, U, V) for torchaudio\n",
        "#     log_probs = log_probs.permute(1, 0, 2, 3)\n",
        "#     log_probs = log_probs.contiguous()\n",
        "\n",
        "#     # Ensure targets are contiguous and correct dtype\n",
        "#     targets = targets.contiguous().to(torch.int32)\n",
        "\n",
        "#     # Compute RNN-T loss\n",
        "#     loss = torchaudio.functional.rnnt_loss(\n",
        "#         log_probs,\n",
        "#         targets,\n",
        "#         input_lengths,\n",
        "#         target_lengths,\n",
        "#         blank=blank,\n",
        "#         reduction='mean'\n",
        "#     )\n",
        "\n",
        "#     return loss"
      ],
      "metadata": {
        "id": "LXG052Jp219E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_epoch(model, dataloader, optimizer):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch_idx, batch in enumerate(dataloader):\n",
        "#         audio = batch['audio']\n",
        "#         text = batch['text']\n",
        "\n",
        "#         # Fix 1: Use the audio_lengths provided by collate_fn\n",
        "#         input_lengths = batch['audio_lengths']\n",
        "\n",
        "#         # Fix 2: Adjust target_lengths to exclude EOS token\n",
        "#         target_lengths = torch.sum(text != tokenizer.token_to_id(\"<pad>\"), dim=1).to(torch.int32)\n",
        "#         target_lengths = target_lengths - 2  # Subtract 1 for EOS token\n",
        "\n",
        "#         # Forward pass\n",
        "#         optimizer.zero_grad()\n",
        "#         logits = model(audio, text[:, :-1]) # Pass unshifted sequence as target\n",
        "#         logits = logits.permute(1, 0, 2, 3)\n",
        "#         # Compute loss\n",
        "#         loss = rnnt_loss(\n",
        "#             logits,\n",
        "#             text[:, 1:].contiguous(), # targets (shifted by 1 to the right)\n",
        "#             input_lengths,\n",
        "#             target_lengths # Subtract 1 for EOS tokens in targets\n",
        "#         )\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "pKD5Qqfu26ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate(model, dataloader):\n",
        "#     model.eval()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in dataloader:\n",
        "#             audio = batch['audio']\n",
        "#             text = batch['text']\n",
        "\n",
        "#             # Fix 1: Use the audio_lengths provided by collate_fn\n",
        "#             input_lengths = batch['audio_lengths']\n",
        "\n",
        "#             # Fix 2: Adjust target_lengths to exclude EOS token\n",
        "#             target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], dtype=torch.int32, device=device)\n",
        "#             target_lengths = target_lengths - 2  # Subtract 1 for EOS token\n",
        "\n",
        "#             logits = model(audio, text[:, :-1]) # Pass unshifted sequence as target\n",
        "\n",
        "#             # Transpose the logits to match the expected shape (T, B, U, V)\n",
        "#             logits = logits.permute(1, 0, 2, 3)\n",
        "\n",
        "#             loss = rnnt_loss(\n",
        "#                 logits.contiguous(),\n",
        "#                 text[:, 1:].contiguous(),\n",
        "#                 input_lengths,\n",
        "#                 target_lengths # Subtract 1 for EOS in targets\n",
        "#             )\n",
        "\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#     return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "w80gsKPg3BPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     train_dataset,\n",
        "#     batch_size=4,\n",
        "#     shuffle=True,\n",
        "#     collate_fn=collate_fn\n",
        "# )\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=4,\n",
        "#     shuffle=False,\n",
        "#     collate_fn=collate_fn\n",
        "# )"
      ],
      "metadata": {
        "id": "p9sKNUfaq72x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training\n",
        "# num_epochs = 5\n",
        "# for epoch in range(num_epochs):\n",
        "#     train_loss = train_epoch(model, train_loader, optimizer)\n",
        "#     val_loss = evaluate(model, test_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "#     print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "XOUrFrPr3EUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference function\n",
        "def predict(model, audio_feature, tokenizer, max_decode_len=100):\n",
        "    model.eval()\n",
        "\n",
        "    # Add batch dimension\n",
        "    audio_feature = audio_feature.unsqueeze(0).to(device)\n",
        "    input_length = torch.tensor([audio_feature.size(1)], device=device)\n",
        "\n",
        "    # Initialize with BOS token\n",
        "    decoded = [tokenizer.token_to_id(\"<bos>\")]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h_enc = model.encoder(audio_feature)  # (1, T, H)\n",
        "\n",
        "        for _ in range(max_decode_len):\n",
        "            # Get last predicted token\n",
        "            last_token = torch.tensor([decoded[-1]], device=device).unsqueeze(0)\n",
        "\n",
        "            # Decoder step\n",
        "            h_dec = model.decoder(last_token)  # (1, 1, H)\n",
        "\n",
        "            # Joint network\n",
        "            logits = model.joint(h_enc, h_dec)  # (1, T, 1, V)\n",
        "            log_probs = torch.nn.functional.log_softmax(logits.squeeze(2), dim=-1)  # (1, T, V)\n",
        "\n",
        "            # Sum over time (approximate)\n",
        "            scores = log_probs.sum(1)  # (1, V)\n",
        "            next_token = scores.argmax(-1).item()\n",
        "\n",
        "            # Stop if EOS is predicted\n",
        "            if next_token == tokenizer.token_to_id(\"<eos>\"):\n",
        "                break\n",
        "\n",
        "            decoded.append(next_token)\n",
        "\n",
        "    # Convert to text\n",
        "    tokens = tokenizer.decode(decoded[1:])  # Skip BOS\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "VhFykH0z2rKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inference\n",
        "test_idx = 0\n",
        "test_audio = features[test_idx]\n",
        "test_text = cleaned_texts[test_idx]\n",
        "\n",
        "predicted_text = predict(model, test_audio, tokenizer)\n",
        "print(f\"Original: {test_text}\")\n",
        "print(f\"Predicted: {predicted_text}\")"
      ],
      "metadata": {
        "id": "KXJLSqV53G8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}