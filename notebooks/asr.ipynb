{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil"
      ],
      "metadata": {
        "id": "eRWwuyT8Bjbk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source_folder = 'checkpoints'\n",
        "# destination_folder = 'datasets/checkpoints'  # Create 'checkpoints' inside 'datasets'\n",
        "\n",
        "# shutil.move(source_folder, destination_folder)"
      ],
      "metadata": {
        "id": "0PQAOeZRBnhI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "Mg_YAjQ9ttKS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "c05ac30a-621f-466d-cd88-fd0c3daedc20"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'datasets'\n",
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-train.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df[:10000]"
      ],
      "metadata": {
        "id": "SPCU8pwmKP0z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[2000:2050]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC",
        "collapsed": true
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85711ebd-2f24-4695-808a-2bc64be7b3f3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-train'\n",
        "\n",
        "train_size = int(0.8 * len(dev_df))\n",
        "train_df = dev_df.iloc[:train_size]\n",
        "test_df = dev_df.iloc[train_size+1:]\n",
        "\n",
        "train_audio_files = [os.path.join(f) for f in train_df['filename']]\n",
        "train_texts = train_df['text'].tolist()\n",
        "\n",
        "test_audio_files = [os.path.join(f) for f in test_df['filename']]\n",
        "test_texts = test_df['text'].tolist()"
      ],
      "metadata": {
        "id": "mzKLy1FmUBqo"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.info()"
      ],
      "metadata": {
        "id": "MbdikRp0M3x6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<blank>\"],\n",
        "    vocab_size=16000,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in train_texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "RQhYjuBSMj0v"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install noisereduce"
      ],
      "metadata": {
        "id": "17IXsOsYyadF"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import noisereduce as nr\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IwEcJmCK_o1j"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_silence(audio, sr, top_db=20, frame_length=2048, hop_length=512):\n",
        "    \"\"\"Remove silent segments from audio using librosa's trim function\"\"\"\n",
        "    # Trim leading and trailing silence\n",
        "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db,\n",
        "                                          frame_length=frame_length,\n",
        "                                          hop_length=hop_length)\n",
        "    return trimmed_audio"
      ],
      "metadata": {
        "id": "lT8wUa8Hq4O2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_paths, target_sr=16000, max_duration=5, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing pipeline for RNN-T\n",
        "    Returns:\n",
        "        log_mel: (batch_size, T, 80) tensor of log Mel spectrograms\n",
        "        lengths: (batch_size,) tensor of actual lengths in frames\n",
        "    \"\"\"\n",
        "    # Initialize mel transform once\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        win_length=400,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_specs = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    # Calculate max_len in frames based on max_duration\n",
        "    max_len = int(max_duration * target_sr / 160)  # hop_length is 160\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Load audio with original sample rate\n",
        "        audio_data, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # ===== SILENCE REMOVAL =====\n",
        "        original_duration = len(audio_data) / sr\n",
        "        audio_data = remove_silence(audio_data, sr)\n",
        "        new_duration = len(audio_data) / sr\n",
        "        silence_removed = original_duration - new_duration\n",
        "\n",
        "        # ===== NOISE REDUCTION =====\n",
        "        # Only attempt noise reduction if we have enough audio left\n",
        "        if len(audio_data) > 0.2 * sr:  # At least 200ms remaining\n",
        "            # Estimate noise from non-silent portions (first 100ms after silence removal)\n",
        "            noise_samples = int(0.1 * sr)\n",
        "            noise_profile = audio_data[:min(noise_samples, len(audio_data))]\n",
        "\n",
        "            reduced_noise = nr.reduce_noise(\n",
        "                y=audio_data,\n",
        "                sr=sr,\n",
        "                y_noise=noise_profile,\n",
        "                stationary=False,\n",
        "                prop_decrease=0.8\n",
        "            )\n",
        "        else:\n",
        "            reduced_noise = audio_data  # Skip if too short after silence removal\n",
        "\n",
        "        # ===== VOLUME NORMALIZATION =====\n",
        "        rms = np.sqrt(np.mean(reduced_noise**2))\n",
        "        current_dBFS = 20 * np.log10(max(rms, 1e-10))\n",
        "        target_dBFS = -20\n",
        "        gain = min(target_dBFS - current_dBFS, 20)\n",
        "\n",
        "        if gain > 1:\n",
        "            amplified_audio = reduced_noise * (10 ** (gain / 20))\n",
        "            amplified_audio = np.clip(amplified_audio, -1.0, 1.0)\n",
        "        else:\n",
        "            amplified_audio = reduced_noise\n",
        "\n",
        "        # ===== RESAMPLING =====\n",
        "        target_sr = 16000\n",
        "        if sr != target_sr:\n",
        "            resampled_audio = librosa.resample(amplified_audio, orig_sr=sr, target_sr=target_sr)\n",
        "        else:\n",
        "            resampled_audio = amplified_audio\n",
        "\n",
        "        sf.write(file_path, resampled_audio, target_sr, format='mp3')\n",
        "        audio_tensor, sr = torchaudio.load(file_path)\n",
        "        audio_tensor = audio_tensor.to(device)\n",
        "        # 3. Mel transform\n",
        "        mel_spec = mel_transform(audio_tensor)  # (channels, n_mels, time)\n",
        "        log_mel = torch.log(mel_spec + 1e-8)\n",
        "        mean = log_mel.mean(dim=2, keepdim=True)\n",
        "        std = log_mel.std(dim=2, keepdim=True) + 1e-8\n",
        "        log_mel = (log_mel - mean) / std\n",
        "        # Permute to (channels, time, n_mels)\n",
        "        log_mel = log_mel.permute(0, 2, 1)\n",
        "        # 4. Padding/trimming\n",
        "        if log_mel.size(1) > max_len:\n",
        "            log_mel = log_mel[:, :max_len+1, :]\n",
        "        else:\n",
        "            pad_amount = max_len + 1 - log_mel.size(1)\n",
        "            log_mel = torch.nn.functional.pad(log_mel, (0, 0, 0, pad_amount))\n",
        "\n",
        "        mel_specs.append(log_mel.squeeze(0))  # Remove channel dimension if single channel\n",
        "        audio_lengths.append(log_mel.size(1))\n",
        "\n",
        "    # Stack into batch (batch_size, T, n_mels)\n",
        "    mel_specs = torch.stack(mel_specs)\n",
        "    audio_lengths = torch.tensor(audio_lengths, dtype=torch.int32)\n",
        "\n",
        "    return mel_specs, audio_lengths"
      ],
      "metadata": {
        "id": "MTcU-suGxili"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, input_dim)\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)   # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # y: (B, U)\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        y = self.dropout(y)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        # Use a more robust approach with a multi-layer network\n",
        "        self.joint = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "\n",
        "        # Get batch size and sequence lengths\n",
        "        batch_size = h_enc.size(0)\n",
        "        T = h_enc.size(1)  # Audio sequence length\n",
        "        U = h_dec.size(1)  # Text sequence length\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "\n",
        "        # Expand to create the full cartesian product for alignment\n",
        "        h_enc = h_enc.expand(-1, -1, U, -1)  # (B, T, U, H)\n",
        "        h_dec = h_dec.expand(-1, T, -1, -1)  # (B, T, U, H)\n",
        "\n",
        "        # Concatenate features\n",
        "        joint = torch.cat([h_enc, h_dec], dim=-1)  # (B, T, U, 2H)\n",
        "\n",
        "        # Apply joint network to get logits\n",
        "        logits = self.joint(joint)  # (B, T, U, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(decoder_dim, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: (B, T, features), y: (B, U)\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "\n",
        "        # Both should now have the same hidden dimension\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_files, texts, tokenizer, max_text_length=100,\n",
        "                 target_sr=16000, max_duration=2):\n",
        "        \"\"\"\n",
        "        Dataset class that handles:\n",
        "        - File paths for batch audio processing\n",
        "        - Text sequences with tokenization\n",
        "        - Automatic audio preprocessing in batches\n",
        "        \"\"\"\n",
        "        self.audio_files = [str(f) for f in audio_files]  # Ensure string paths\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.target_sr = target_sr\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns single item with:\n",
        "        - audio_file: Path for batch processing\n",
        "        - text: Raw text string for tokenization in collate\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'audio_file': self.audio_files[idx],\n",
        "            'text': self.texts[idx],\n",
        "            'tokenizer': self.tokenizer,  # Pass tokenizer for collate_fn\n",
        "            'max_text_length': self.max_text_length\n",
        "        }\n",
        "\n",
        "def collate_fn(batch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Custom collate function that:\n",
        "    1. Processes audio files in batch using preprocess_for_rnnt_torchaudio\n",
        "    2. Tokenizes and pads text sequences\n",
        "    3. Handles audio length preservation\n",
        "    \"\"\"\n",
        "    # Extract batch components\n",
        "    audio_files = [item['audio_file'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Batch process audio files\n",
        "    audio_features, audio_lengths = preprocess_for_rnnt_torchaudio(\n",
        "        audio_files,\n",
        "        target_sr=batch[0].get('target_sr', 16000),\n",
        "        max_duration=batch[0].get('max_duration', 2),\n",
        "        device=device\n",
        "    )  # (B, T, 80)\n",
        "\n",
        "    target_lengths = [item['max_text_length'] - 2 for item in batch]\n",
        "\n",
        "    # Tokenize and pad texts\n",
        "    text_sequences = []\n",
        "    for item in batch:\n",
        "        encoding = item['tokenizer'].encode(item['text'])\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add special tokens\n",
        "        text_ids = [\n",
        "            item['tokenizer'].token_to_id(\"<bos>\")\n",
        "        ] + text_ids + [\n",
        "            item['tokenizer'].token_to_id(\"<eos>\")\n",
        "        ]\n",
        "\n",
        "        # Pad or truncate\n",
        "        max_len = item['max_text_length']\n",
        "        if len(text_ids) < max_len:\n",
        "            text_ids = text_ids + [item['tokenizer'].token_to_id(\"<pad>\")] * (max_len - len(text_ids))\n",
        "        else:\n",
        "            text_ids = text_ids[:max_len]\n",
        "\n",
        "        text_sequences.append(torch.tensor(text_ids, dtype=torch.int32))\n",
        "\n",
        "    target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.int32)\n",
        "    # Stack text sequences\n",
        "    text_tensor = torch.stack(text_sequences)\n",
        "\n",
        "    return {\n",
        "        'audio': audio_features.to(device),  # (B, T, 80)\n",
        "        'text': text_tensor.to(device),     # (B, U_max)\n",
        "        'target_lengths': target_lengths_tensor.to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, device=device) # (B,)\n",
        "    }"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision=True):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    # Enable mixed precision for faster training and lower memory usage\n",
        "    scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get batch data\n",
        "        audio = batch['audio']  # (B, T, 80)\n",
        "        text = batch['text']    # (B, U)\n",
        "\n",
        "        # Get lengths\n",
        "        input_lengths = batch['audio_lengths']\n",
        "        target_lengths = batch['target_lengths']\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mixed_precision:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Forward pass - encoder\n",
        "                encoder_output = model.encoder(audio)\n",
        "\n",
        "                # Forward pass - decoder and joint\n",
        "                h_dec = model.decoder(text[:, :-1])  # (B, U-1, H)\n",
        "                logits = model.joint(encoder_output, h_dec)  # (B, T, U-1, V)\n",
        "\n",
        "                # For targets, use labels shifted right (exclude BOS)\n",
        "                targets = text[:, 1:-1].contiguous()  # Remove only BOS for targets\n",
        "\n",
        "                # Compute loss\n",
        "                blank_id = tokenizer.token_to_id('<blank>')\n",
        "                loss = torchaudio.functional.rnnt_loss(\n",
        "                    logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                    targets=targets,\n",
        "                    logit_lengths=input_lengths,\n",
        "                    target_lengths=target_lengths,\n",
        "                    blank=blank_id,\n",
        "                    reduction='mean'\n",
        "                )\n",
        "\n",
        "            # Backward pass with scaled gradients\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Lower clip threshold\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Standard training without mixed precision\n",
        "            encoder_output = model.encoder(audio)\n",
        "            h_dec = model.decoder(text[:, :-1])\n",
        "            logits = model.joint(encoder_output, h_dec)\n",
        "\n",
        "            targets = text[:, 1:-1].contiguous()\n",
        "\n",
        "            blank_id = tokenizer.token_to_id('<blank>')\n",
        "            loss = torchaudio.functional.rnnt_loss(\n",
        "                logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                targets=targets,\n",
        "                logit_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=blank_id,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update learning rate with scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    return total_loss / steps"
      ],
      "metadata": {
        "id": "AbKbvmzwtkc3"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "IIa9x7nMhF5c"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in train_audio_files],\n",
        "    texts=train_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in test_audio_files],\n",
        "    texts=test_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        ")"
      ],
      "metadata": {
        "id": "Lhzo3JhgKWxZ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'checkpoints/rnnt_epoch_6.pt'\n",
        "checkpoint = torch.load(checkpoint_path)"
      ],
      "metadata": {
        "id": "z0KRKbxDCIdV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5,  # L2 regularization\n",
        "        betas=(0.9, 0.999))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=5e-4,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=5,\n",
        "        pct_start=0.2,  # Warm-up for first 10% of training\n",
        "        div_factor=10,  # Initial LR = max_lr/div_factor\n",
        "        final_div_factor=100,  # Final LR = initial_lr/final_div_factor\n",
        "        anneal_strategy='linear'\n",
        "    )\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    # Train for one epoch\n",
        "    train_loss = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        mixed_precision=True\n",
        "    )\n",
        "\n",
        "    print(f\"Train Loss {epoch}: {train_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if save_dir: # Use save_dir instead of args.save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(save_dir, f\"rnnt_epoch_{epoch+7}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union"
      ],
      "metadata": {
        "id": "r-8zH-OXGqFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rnnt_beam_search(\n",
        "    model: torch.nn.Module,\n",
        "    audio_features: torch.Tensor,\n",
        "    tokenizer: object,\n",
        "    device: Union[str, torch.device],\n",
        "    beam_width: int = 5,\n",
        "    max_decode_len: int = 100,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Performs beam search decoding for an RNN-T model.\n",
        "\n",
        "    Args:\n",
        "        model: The RNN-T model\n",
        "        audio_features: The audio features tensor\n",
        "        tokenizer: The tokenizer object\n",
        "        device: The device to run inference on\n",
        "        beam_width: Width of the beam search\n",
        "        max_decode_len: Maximum decoding length\n",
        "\n",
        "    Returns:\n",
        "        The best decoded transcript\n",
        "    \"\"\"\n",
        "    # Add batch dimension if missing (shape: [1, T, D])\n",
        "    if len(audio_features.shape) == 2:\n",
        "        audio_features = audio_features.unsqueeze(0).to(device)\n",
        "    else:\n",
        "        audio_features = audio_features.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode the audio features\n",
        "        h_enc = model.encoder(audio_features)  # [1, T, H]\n",
        "\n",
        "        # Initialize the beam\n",
        "        # Each beam element: (negative log prob, sequence, decoder state)\n",
        "        bos_token = tokenizer.token_to_id(\"<bos>\")\n",
        "        eos_token = tokenizer.token_to_id(\"<eos>\")\n",
        "        blank_token = tokenizer.token_to_id(\"<blank>\")\n",
        "\n",
        "        # Initialize beam with just the beginning token\n",
        "        current_idx = torch.tensor([[bos_token]], device=device)\n",
        "        initial_decoder_output = model.decoder(current_idx)  # [1, 1, H]\n",
        "\n",
        "        # Start with a single beam\n",
        "        beams = [(0.0, [bos_token], initial_decoder_output)]\n",
        "        finished_beams = []\n",
        "\n",
        "        # Main beam search loop\n",
        "        for step in range(max_decode_len):\n",
        "            candidates = []\n",
        "\n",
        "            # Expand each current beam\n",
        "            for log_prob, sequence, decoder_output in beams:\n",
        "                # Skip if sequence already ended\n",
        "                if sequence[-1] == eos_token:\n",
        "                    finished_beams.append((log_prob, sequence))\n",
        "                    continue\n",
        "\n",
        "                # Run joint network on the encoded audio and last decoder output\n",
        "                logits = model.joint(h_enc, decoder_output)  # [1, T, 1, V]\n",
        "\n",
        "                # Sum over time dimension for alignment scores\n",
        "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "                scores = log_probs.sum(dim=1).squeeze(1)  # [1, V]\n",
        "\n",
        "                # Get top-k tokens\n",
        "                top_log_probs, top_indices = scores.topk(beam_width)\n",
        "\n",
        "                # Add candidates to our list\n",
        "                for i in range(beam_width):\n",
        "                    token_id = top_indices[0, i].item()\n",
        "                    token_log_prob = top_log_probs[0, i].item()\n",
        "\n",
        "                    # Skip blank tokens in final output but consider them for alignment\n",
        "                    if token_id == blank_token:\n",
        "                        candidates.append((log_prob + token_log_prob, sequence.copy(), decoder_output))\n",
        "                    else:\n",
        "                        new_sequence = sequence.copy() + [token_id]\n",
        "\n",
        "                        # Update decoder state with new token\n",
        "                        new_token = torch.tensor([[token_id]], device=device)\n",
        "                        new_decoder_output = model.decoder(new_token)\n",
        "\n",
        "                        candidates.append((log_prob + token_log_prob, new_sequence, new_decoder_output))\n",
        "\n",
        "            # Keep only the best beam_width candidates\n",
        "            candidates.sort(key=lambda x: x[0], reverse=True)  # Sort by log probability (higher is better)\n",
        "            beams = candidates[:beam_width]\n",
        "\n",
        "            # Early stopping if all beams have finished\n",
        "            if all(beam[1][-1] == eos_token for beam in beams):\n",
        "                break\n",
        "\n",
        "        # Add any unfinished beams to finished_beams\n",
        "        for log_prob, sequence, _ in beams:\n",
        "            if sequence[-1] != eos_token:\n",
        "                finished_beams.append((log_prob, sequence))\n",
        "\n",
        "        best_sequence = None\n",
        "        # print(\"Number of beams: \", finished_beams)\n",
        "        # Select the most likely beam\n",
        "        if finished_beams:\n",
        "            best_beam = max(finished_beams, key=lambda x: x[0])\n",
        "            best_sequence = best_beam[1]\n",
        "        else:\n",
        "            best_sequence = beams[0][1]  # Best unfinished beam\n",
        "\n",
        "        # Remove special tokens (BOS, EOS) for the final output\n",
        "        filtered_sequence = [token for token in best_sequence\n",
        "                            if token != bos_token and token != eos_token]\n",
        "\n",
        "        return tokenizer.decode(filtered_sequence)"
      ],
      "metadata": {
        "id": "t7iEpIvbLUOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Specify the directory where checkpoints are saved\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Get the last checkpoint file\n",
        "last_checkpoint_file = 'rnnt_epoch_3.pt'\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint_path = os.path.join(save_dir, last_checkpoint_file)\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Create a new instance of your model (if needed)\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "\n",
        "# Load the model state dictionary from the checkpoint\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Load the optimizer state dictionary (optional)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "\n",
        "print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "print(f\"Loss: {checkpoint['loss']}\")"
      ],
      "metadata": {
        "id": "2vJUeqkrgVvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install jiwer"
      ],
      "metadata": {
        "id": "Xj1DFE1kL3RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer"
      ],
      "metadata": {
        "id": "XLpAVrYGL0EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model, tokenizer, and features\n",
        "model.eval()  # Your RNN-T model  # Shape: [T, D]\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "    for audio in test_audio_files[1000:1010]:\n",
        "        audio_features, _ = preprocess_for_rnnt_torchaudio([os.path.join(audio_directory, audio)])\n",
        "        text_beam = predict_rnnt_beam_search(model, audio_features, tokenizer, \"cuda\")\n",
        "        wer = jiwer.wer(test_texts[i], text_beam)\n",
        "        print(f\"Beam: {text_beam}\")\n",
        "        print(f\"Real text: {test_texts[i]}\")\n",
        "        print(f\"wer: {wer:.4f}\")\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "3WcBnC1yqvIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r checkpoints.zip checkpoints"
      ],
      "metadata": {
        "id": "Qq77BA25Oiko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoints.zip')"
      ],
      "metadata": {
        "id": "i7MLURLXOmJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}