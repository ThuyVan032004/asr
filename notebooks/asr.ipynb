{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37647df4-c7bd-44cd-ecfc-ee9496bfce92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle==1.5.12 in /usr/local/lib/python3.11/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (1.17.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle==1.5.12) (2.3.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle==1.5.12) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle==1.5.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle==1.5.12) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06778710-bc04-4d14-b493-240cfce1b5bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "wsv_WL4WdjDB",
        "outputId": "cff3a0ad-9b94-436b-b3e8-ece13c121db2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading common-voice.zip to /content/datasets\n",
            "  3% 345M/12.0G [00:17<10:06, 20.7MB/s]\n",
            "User cancelled operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "35ce24e3-0c07-4ffb-e3d7-0b84a661b970"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZm5CsqJgsv5",
        "outputId": "d5093fc6-58b4-433c-d470-e905fb8a8232"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  common-voice.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of common-voice.zip or\n",
            "        common-voice.zip.zip, and cannot find common-voice.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-dev.csv')\n",
        "dev_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fyjBBRLPg_se",
        "outputId": "60ee9e69-bed1-414e-8dd9-ff63ed9c7326"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         filename  \\\n",
              "0  cv-valid-dev/sample-000000.mp3   \n",
              "1  cv-valid-dev/sample-000001.mp3   \n",
              "2  cv-valid-dev/sample-000002.mp3   \n",
              "3  cv-valid-dev/sample-000003.mp3   \n",
              "4  cv-valid-dev/sample-000004.mp3   \n",
              "\n",
              "                                                text  up_votes  down_votes  \\\n",
              "0  be careful with your prognostications said the...         1           0   \n",
              "1  then why should they be surprised when they se...         2           0   \n",
              "2  a young arab also loaded down with baggage ent...         2           0   \n",
              "3  i thought that everything i owned would be des...         3           0   \n",
              "4  he moved about invisible but everyone could he...         1           0   \n",
              "\n",
              "        age  gender   accent  duration  \n",
              "0       NaN     NaN      NaN       NaN  \n",
              "1       NaN     NaN      NaN       NaN  \n",
              "2       NaN     NaN      NaN       NaN  \n",
              "3       NaN     NaN      NaN       NaN  \n",
              "4  fourties  female  england       NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9c0bae4-3955-4f7a-b8e2-dff533be349f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>up_votes</th>\n",
              "      <th>down_votes</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>accent</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cv-valid-dev/sample-000000.mp3</td>\n",
              "      <td>be careful with your prognostications said the...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cv-valid-dev/sample-000001.mp3</td>\n",
              "      <td>then why should they be surprised when they se...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cv-valid-dev/sample-000002.mp3</td>\n",
              "      <td>a young arab also loaded down with baggage ent...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cv-valid-dev/sample-000003.mp3</td>\n",
              "      <td>i thought that everything i owned would be des...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cv-valid-dev/sample-000004.mp3</td>\n",
              "      <td>he moved about invisible but everyone could he...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>fourties</td>\n",
              "      <td>female</td>\n",
              "      <td>england</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9c0bae4-3955-4f7a-b8e2-dff533be349f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9c0bae4-3955-4f7a-b8e2-dff533be349f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9c0bae4-3955-4f7a-b8e2-dff533be349f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-20098065-c33a-4667-a627-2df33d7d6cdf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-20098065-c33a-4667-a627-2df33d7d6cdf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-20098065-c33a-4667-a627-2df33d7d6cdf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dev_df",
              "summary": "{\n  \"name\": \"dev_df\",\n  \"rows\": 4076,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4076,\n        \"samples\": [\n          \"cv-valid-dev/sample-001749.mp3\",\n          \"cv-valid-dev/sample-002053.mp3\",\n          \"cv-valid-dev/sample-000538.mp3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2410,\n        \"samples\": [\n          \"their horses cried out and all their weapons were filled with sand\",\n          \"and look how many things the wind already knew how to do\",\n          \"i done my best\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"up_votes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 1,\n        \"max\": 366,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          31,\n          11,\n          62\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"down_votes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 21,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          11,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"teens\",\n          \"sixties\",\n          \"fourties\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"female\",\n          \"male\",\n          \"other\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"england\",\n          \"us\",\n          \"scotland\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fe0830-123a-453f-8034-51b15e24c673"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4076 entries, 0 to 4075\n",
            "Data columns (total 8 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   filename    4076 non-null   object \n",
            " 1   text        4076 non-null   object \n",
            " 2   up_votes    4076 non-null   int64  \n",
            " 3   down_votes  4076 non-null   int64  \n",
            " 4   age         1528 non-null   object \n",
            " 5   gender      1540 non-null   object \n",
            " 6   accent      1350 non-null   object \n",
            " 7   duration    0 non-null      float64\n",
            "dtypes: float64(1), int64(2), object(5)\n",
            "memory usage: 254.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euyV6PTthVWH",
        "outputId": "ca316d9e-8fee-4d0a-cf71-6d9853176f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "import os\n",
        "\n",
        "def display_audio(audio_path):\n",
        "  display(Audio(audio_path))\n",
        "\n",
        "audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "for audio_file in audio_files[:5]:\n",
        "  audio_path = os.path.join(audio_directory, audio_file)\n",
        "  display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = os.path.join(audio_directory, \"sample-000000.mp3\")\n",
        "display_audio(audio_path)"
      ],
      "metadata": {
        "id": "eoGLOXhDqYa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = torch.utils.data.random_split(dev_df, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "q3NtLOeJl0Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_path, target_sr=16000, max_duration=5):\n",
        "    \"\"\"GPU-accelerated preprocessing pipeline for RNN-T using TorchAudio\"\"\"\n",
        "\n",
        "    # 1. Load audio directly to GPU (PyTorch 2.0+)\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    # 2. Resample if needed\n",
        "    if sr != target_sr:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "    # 3. Trim silence (VAD)\n",
        "    waveform = torchaudio.functional.vad(waveform, sample_rate=target_sr, trigger_level=20)\n",
        "\n",
        "    # 4. Peak normalization (GPU)\n",
        "    waveform = torch.nn.functional.normalize(waveform, dim=-1)\n",
        "\n",
        "    # 5. Fixed-length padding/cropping\n",
        "    max_samples = target_sr * max_duration\n",
        "    if waveform.size(-1) > max_samples:\n",
        "        waveform = waveform[..., :max_samples]\n",
        "    else:\n",
        "        pad_amount = max_samples - waveform.size(-1)\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "    # 6. Extract Log-Mel features (GPU)\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_spec = mel_transform(waveform)\n",
        "    log_mel = torch.log(mel_spec + 1e-6)  # (1, 80, T)\n",
        "\n",
        "    return log_mel.squeeze(0).T  # (T, 80)"
      ],
      "metadata": {
        "id": "Jb9YzYmZKfFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-dev'\n",
        "audio_files = [train_df.dataset.loc[train_df.indices[i], 'filename'] for i in range(len(train_df))]"
      ],
      "metadata": {
        "id": "0NOcj3K8lzI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "for audio in audio_files:\n",
        "  audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "  features.append(audio_feature)"
      ],
      "metadata": {
        "id": "ewSOst3wmBmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = train_df[\"text\"].tolist()"
      ],
      "metadata": {
        "id": "waufdibonnWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:5]"
      ],
      "metadata": {
        "id": "OMtg8Eb3_EnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning\n",
        "cleaned_texts = [clean_text(t) for t in texts if isinstance(t, str)]"
      ],
      "metadata": {
        "id": "cfVrlcpd06oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts[:5]"
      ],
      "metadata": {
        "id": "syWTuNa25qSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Simpler pre-tokenizer (preserve apostrophes)\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),\n",
        "    pre_tokenizers.Split(\"'\", behavior=\"isolated\")  # Special handling for apostrophes\n",
        "])\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
        "    vocab_size=3800,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")\n",
        "\n",
        "# Test encoding\n",
        "sample_text = \"Dr O'Neill 's patient said We 'll check again tomorrow\"\n",
        "encoding = tokenizer.encode(sample_text.lower())\n",
        "print(\"Tokens:\", encoding.tokens)\n",
        "print(\"IDs:\", encoding.ids)"
      ],
      "metadata": {
        "id": "fcn_7c7v3Xe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.linear(x)    # (B, T, H)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IR3gNzgsIoEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, y):\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        return y"
      ],
      "metadata": {
        "id": "V5Ima62TIqXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "        out = torch.tanh(h_enc + h_dec)  # (B, T, U, H)\n",
        "        return self.linear(out)  # (B, T, U, V)"
      ],
      "metadata": {
        "id": "5K2duKk7IsRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(encoder_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "9ZvXpa3aIuE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue from where the notebook left off\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, texts, tokenizer, max_text_length=100):\n",
        "        self.features = features\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get audio features\n",
        "        audio_feature = self.features[idx]\n",
        "\n",
        "        # Get and tokenize text\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode(text)\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add BOS and EOS tokens\n",
        "        text_ids = [tokenizer.token_to_id(\"<bos>\")] + text_ids + [tokenizer.token_to_id(\"<eos>\")]\n",
        "\n",
        "        # Pad text sequence\n",
        "        if len(text_ids) < self.max_text_length:\n",
        "            pad_amount = self.max_text_length - len(text_ids)\n",
        "            text_ids = text_ids + [tokenizer.token_to_id(\"<pad>\")] * pad_amount\n",
        "        else:\n",
        "            text_ids = text_ids[:self.max_text_length]\n",
        "\n",
        "        return {\n",
        "            'audio': audio_feature,\n",
        "            'text': torch.tensor(text_ids, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "3K1AB6iTIvDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    features=[features[i] for i in train_df.indices],\n",
        "    texts=[cleaned_texts[i] for i in train_df.indices],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = AudioTextDataset(\n",
        "    features=[features[i] for i in test_df.indices],\n",
        "    texts=[cleaned_texts[i] for i in test_df.indices],\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "rMMOxn6d2v_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "def collate_fn(batch):\n",
        "    # Pad audio features to same length\n",
        "    audio_features = [item['audio'] for item in batch]\n",
        "    max_audio_len = max(f.size(0) for f in audio_features)\n",
        "\n",
        "    padded_audio = []\n",
        "    for f in audio_features:\n",
        "        pad_amount = max_audio_len - f.size(0)\n",
        "        if pad_amount > 0:\n",
        "            f = torch.nn.functional.pad(f, (0, 0, 0, pad_amount), value=0)\n",
        "        padded_audio.append(f)\n",
        "\n",
        "    audio_tensor = torch.stack(padded_audio)\n",
        "    text_tensor = torch.stack([item['text'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'audio': audio_tensor.to(device),\n",
        "        'text': text_tensor.to(device)\n",
        "    }\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "83SDmsXp2y_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "model = RNNTransducer(vocab_size).to(device)\n",
        "\n",
        "# Loss function (RNN-T loss)\n",
        "def rnnt_loss(logits, targets, input_lengths, target_lengths, blank=0):\n",
        "    # logits: (B, T, U, V)\n",
        "    # targets: (B, U)\n",
        "    # input_lengths: (B,) - length of each audio sequence\n",
        "    # target_lengths: (B,) - length of each text sequence\n",
        "\n",
        "    # Convert to log-probs\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Transpose for torchaudio's expectation (T, B, U, V)\n",
        "    log_probs = log_probs.permute(1, 0, 2, 3)\n",
        "\n",
        "    # Compute RNN-T loss\n",
        "    loss = torchaudio.functional.rnnt_loss(\n",
        "        log_probs,\n",
        "        targets,\n",
        "        input_lengths,\n",
        "        target_lengths,\n",
        "        blank=blank,\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "LXG052Jp219E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "def train_epoch(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        audio = batch['audio']\n",
        "        text = batch['text']\n",
        "\n",
        "        # Get input/target lengths\n",
        "        input_lengths = torch.tensor([a.size(0) for a in audio], device=device)\n",
        "        target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(audio, text[:, :-1])  # Exclude EOS token for decoder input\n",
        "\n",
        "        # Compute loss\n",
        "        loss = rnnt_loss(\n",
        "            logits,\n",
        "            text[:, 1:],  # Exclude BOS token for targets\n",
        "            input_lengths,\n",
        "            target_lengths - 1  # Subtract 1 because we excluded BOS\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "pKD5Qqfu26ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            audio = batch['audio']\n",
        "            text = batch['text']\n",
        "\n",
        "            input_lengths = torch.tensor([a.size(0) for a in audio], device=device)\n",
        "            target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], device=device)\n",
        "\n",
        "            logits = model(audio, text[:, :-1])\n",
        "\n",
        "            loss = rnnt_loss(\n",
        "                logits,\n",
        "                text[:, 1:],\n",
        "                input_lengths,\n",
        "                target_lengths - 1\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "w80gsKPg3BPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer)\n",
        "    val_loss = evaluate(model, test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "XOUrFrPr3EUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference function\n",
        "def predict(model, audio_feature, tokenizer, max_decode_len=100):\n",
        "    model.eval()\n",
        "\n",
        "    # Add batch dimension\n",
        "    audio_feature = audio_feature.unsqueeze(0).to(device)\n",
        "    input_length = torch.tensor([audio_feature.size(1)], device=device)\n",
        "\n",
        "    # Initialize with BOS token\n",
        "    decoded = [tokenizer.token_to_id(\"<bos>\")]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h_enc = model.encoder(audio_feature)  # (1, T, H)\n",
        "\n",
        "        for _ in range(max_decode_len):\n",
        "            # Get last predicted token\n",
        "            last_token = torch.tensor([decoded[-1]], device=device).unsqueeze(0)\n",
        "\n",
        "            # Decoder step\n",
        "            h_dec = model.decoder(last_token)  # (1, 1, H)\n",
        "\n",
        "            # Joint network\n",
        "            logits = model.joint(h_enc, h_dec)  # (1, T, 1, V)\n",
        "            log_probs = torch.nn.functional.log_softmax(logits.squeeze(2), dim=-1)  # (1, T, V)\n",
        "\n",
        "            # Sum over time (approximate)\n",
        "            scores = log_probs.sum(1)  # (1, V)\n",
        "            next_token = scores.argmax(-1).item()\n",
        "\n",
        "            # Stop if EOS is predicted\n",
        "            if next_token == tokenizer.token_to_id(\"<eos>\"):\n",
        "                break\n",
        "\n",
        "            decoded.append(next_token)\n",
        "\n",
        "    # Convert to text\n",
        "    tokens = tokenizer.decode(decoded[1:])  # Skip BOS\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "VhFykH0z2rKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inference\n",
        "test_idx = 0\n",
        "test_audio = features[test_idx]\n",
        "test_text = cleaned_texts[test_idx]\n",
        "\n",
        "predicted_text = predict(model, test_audio, tokenizer)\n",
        "print(f\"Original: {test_text}\")\n",
        "print(f\"Predicted: {predicted_text}\")"
      ],
      "metadata": {
        "id": "KXJLSqV53G8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}