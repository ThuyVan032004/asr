{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "wsv_WL4WdjDB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0vNVrDnO37",
        "outputId": "e11ade73-e5fa-48fa-da74-7a76822e8ae6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-dev.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pydub"
      ],
      "metadata": {
        "id": "euyV6PTthVWH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[:5]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91016a8f-b440-41d9-c391-6fd2761f43c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = torch.utils.data.random_split(dev_df, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "q3NtLOeJl0Nm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_path, target_sr=16000, max_duration=3):\n",
        "    \"\"\"GPU-accelerated preprocessing pipeline for RNN-T using TorchAudio\"\"\"\n",
        "\n",
        "    # 1. Load audio directly to GPU (PyTorch 2.0+)\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    # 2. Resample if needed\n",
        "    if sr != target_sr:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "    # 3. Trim silence (VAD)\n",
        "    waveform = torchaudio.functional.vad(waveform, sample_rate=target_sr, trigger_level=20)\n",
        "\n",
        "    # 4. Peak normalization (GPU)\n",
        "    waveform = torch.nn.functional.normalize(waveform, dim=-1)\n",
        "\n",
        "    # 5. Fixed-length padding/cropping\n",
        "    max_samples = target_sr * max_duration\n",
        "    if waveform.size(-1) > max_samples:\n",
        "        waveform = waveform[..., :max_samples]\n",
        "    else:\n",
        "        pad_amount = max_samples - waveform.size(-1)\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "\n",
        "    # 6. Extract Log-Mel features (GPU)\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_spec = mel_transform(waveform)\n",
        "    log_mel = torch.log(mel_spec + 1e-6)  # (1, 80, T)\n",
        "\n",
        "    return log_mel.squeeze(0).T  # (T, 80)"
      ],
      "metadata": {
        "id": "Jb9YzYmZKfFU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-dev'\n",
        "audio_files = [train_df.dataset.loc[train_df.indices[i], 'filename'] for i in range(len(train_df))]"
      ],
      "metadata": {
        "id": "0NOcj3K8lzI4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "for audio in audio_files:\n",
        "  audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "  features.append(audio_feature)"
      ],
      "metadata": {
        "id": "ewSOst3wmBmB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [dev_df.loc[idx, 'text'] for idx in train_df.indices]"
      ],
      "metadata": {
        "id": "waufdibonnWz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts[:5]"
      ],
      "metadata": {
        "id": "OMtg8Eb3_EnS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Keep original case - REMOVED .lower()\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning\n",
        "cleaned_texts = [clean_text(t) for t in texts if isinstance(t, str)]"
      ],
      "metadata": {
        "id": "cfVrlcpd06oC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaned_texts[:5]"
      ],
      "metadata": {
        "id": "syWTuNa25qSn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
        "    vocab_size=6150,  # Increased for better word coverage\n",
        "    min_frequency=3,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load tokenizer\n",
        "# tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")\n",
        "\n",
        "# # Test encoding\n",
        "# sample_text = \"hello mr. sunshine!\"\n",
        "# cleaned_sample = clean_text(sample_text)\n",
        "# print(\"Cleaned sample: \", cleaned_sample)\n",
        "# encoding = tokenizer.encode(sample_text.lower())\n",
        "# print(\"Tokens:\", encoding.tokens)\n",
        "# print(\"Length: \", len(encoding.tokens))\n",
        "# print(\"IDs:\", encoding.ids)\n",
        "# print(\"Length: \", len(encoding.ids))"
      ],
      "metadata": {
        "id": "fcn_7c7v3Xe3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"vocal size: \", tokenizer.get_vocab_size())"
      ],
      "metadata": {
        "id": "5soCVvhQadZn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "        x = self.linear(x)    # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, y):\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "        out = torch.tanh(h_enc + h_dec)  # (B, T, U, H)\n",
        "        return self.linear(out)  # (B, T, U, V)\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(encoder_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits.contiguous()\n",
        "\n",
        "# Continue from where the notebook left off\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, texts, tokenizer, max_text_length=200):\n",
        "        self.features = features\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get audio features\n",
        "        audio_feature = self.features[idx]\n",
        "\n",
        "        # Get and tokenize text\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode(text)\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Add BOS and EOS tokens\n",
        "        text_ids = [tokenizer.token_to_id(\"<bos>\")] + text_ids + [tokenizer.token_to_id(\"<eos>\")]\n",
        "\n",
        "        # Pad text sequence\n",
        "        if len(text_ids) < self.max_text_length:\n",
        "            pad_amount = self.max_text_length - len(text_ids)\n",
        "            text_ids = text_ids + [tokenizer.token_to_id(\"<pad>\")] * pad_amount\n",
        "        else:\n",
        "            text_ids = text_ids[:self.max_text_length]\n",
        "\n",
        "        return {\n",
        "            'audio': audio_feature,\n",
        "            'text': torch.tensor(text_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    audio_features = [item['audio'] for item in batch]\n",
        "    max_audio_len = max(f.size(0) for f in audio_features)\n",
        "\n",
        "    padded_audio = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    for f in audio_features:\n",
        "        original_length = f.size(0)  # <-- THIS IS CRITICAL\n",
        "        audio_lengths.append(original_length)\n",
        "        pad_amount = max_audio_len - original_length\n",
        "        if pad_amount > 0:\n",
        "            f = torch.nn.functional.pad(f, (0, 0, 0, pad_amount), value=0)\n",
        "        padded_audio.append(f)\n",
        "\n",
        "    return {\n",
        "        'audio': torch.stack(padded_audio).to(device),\n",
        "        'text': torch.stack([item['text'] for item in batch]).to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, dtype=torch.int32, device=device)\n",
        "    }"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    features=features,\n",
        "    texts=cleaned_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cleaned_texts = [clean_text(dev_df.loc[idx, 'text']) for idx in test_df.indices]\n",
        "test_features = []\n",
        "audio_directory = 'cv-valid-dev'\n",
        "audio_files = [test_df.dataset.loc[test_df.indices[i], 'filename'] for i in range(len(test_df))]\n",
        "for audio in audio_files:\n",
        "  audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "  test_features.append(audio_feature)\n",
        "\n",
        "test_dataset = AudioTextDataset(\n",
        "    features=test_features,\n",
        "    texts=test_cleaned_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rnnt_loss(logits, targets, input_lengths, target_lengths, blank=0):\n",
        "    \"\"\"RNN-T loss with guaranteed correct batch dimensions\"\"\"\n",
        "    # Convert to log-probs\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # Ensure correct properties\n",
        "    log_probs = log_probs.contiguous()\n",
        "    targets = targets.contiguous().to(torch.int32)\n",
        "    input_lengths = input_lengths.contiguous().to(torch.int32)\n",
        "    target_lengths = target_lengths.contiguous().to(torch.int32)\n",
        "\n",
        "    # CRITICAL FIX: Input lengths should be the actual time lengths (not padded)\n",
        "    # For your case where all audios are padded to 301 frames:\n",
        "    # input_lengths should be the original lengths before padding\n",
        "    # If you don't have original lengths, use:\n",
        "    # input_lengths = torch.full((log_probs.size(1),), log_probs.size(0), dtype=torch.int32, device=log_probs.device)\n",
        "\n",
        "    # Final validation\n",
        "    B = log_probs.size(1)\n",
        "    assert input_lengths.shape == (B,), f\"Input lengths must be shape ({B},), got {input_lengths.shape}\"\n",
        "    assert targets.shape[0] == B, f\"Targets batch size mismatch: {targets.shape[0]} vs {B}\"\n",
        "    assert target_lengths.shape == (B,), f\"Target lengths must be shape ({B},), got {target_lengths.shape}\"\n",
        "\n",
        "    return torchaudio.functional.rnnt_loss(\n",
        "        log_probs=log_probs,\n",
        "        targets=targets,\n",
        "        logit_lengths=input_lengths,\n",
        "        target_lengths=target_lengths,\n",
        "        blank=blank,\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get batch data\n",
        "        audio = batch['audio']  # (B, T, 80)\n",
        "        text = batch['text']    # (B, U)\n",
        "\n",
        "        # Calculate lengths - MUST use original lengths before padding\n",
        "        input_lengths = batch['audio_lengths'].to(device)  # From collate_fn\n",
        "        target_lengths = (text[:, 1:] != tokenizer.token_to_id(\"<pad>\")).sum(dim=1).to(torch.int32).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(audio, text[:, :-1])  # (B, T, U-1, V)\n",
        "\n",
        "        # Permute to (T, B, U-1, V)\n",
        "        logits = logits.permute(1, 0, 2, 3).contiguous()\n",
        "\n",
        "        # Debug prints\n",
        "        print(f\"\\nBatch {batch_idx} shapes:\")\n",
        "        print(f\"Logits: {logits.shape} (T,B,U,V)\")\n",
        "        print(f\"Targets: {text[:, 1:].shape} (B,U-1)\")\n",
        "        print(f\"Input lengths: {input_lengths.cpu().numpy()} (should be original lengths)\")\n",
        "        print(f\"Target lengths: {target_lengths.cpu().numpy()}\")\n",
        "\n",
        "        # Compute loss\n",
        "        loss = compute_rnnt_loss(\n",
        "            logits=logits,\n",
        "            targets=text[:, 1:].contiguous(),\n",
        "            input_lengths=input_lengths,\n",
        "            target_lengths=target_lengths,\n",
        "            blank=tokenizer.token_to_id(\"<pad>\")\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            audio = batch['audio']\n",
        "            text = batch['text']\n",
        "\n",
        "            input_lengths = batch['audio_lengths'].to(device)\n",
        "            target_lengths = (text[:, 1:] != tokenizer.token_to_id(\"<pad>\")).sum(dim=1).to(torch.int32).to(device)\n",
        "\n",
        "            logits = model(audio, text[:, :-1])  # (B, T, U-1, V)\n",
        "            logits = logits.permute(1, 0, 2, 3).contiguous()\n",
        "\n",
        "            loss = compute_rnnt_loss(\n",
        "                logits=logits,\n",
        "                targets=text[:, 1:].contiguous(),\n",
        "                input_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=tokenizer.token_to_id(\"<pad>\")\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "7YhcqMHqEixr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer)\n",
        "    val_loss = evaluate(model, test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020",
        "outputId": "d1d8191a-567a-4b6d-95af-e6c9143440c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0 shapes:\n",
            "Audio features: torch.Size([2, 301, 80])\n",
            "Text tokens: torch.Size([2, 200])\n",
            "Logits: torch.Size([301, 2, 199, 6142])\n",
            "Targets: torch.Size([2, 199])\n",
            "Input lengths: [301 301]\n",
            "Target lengths: [ 2 14]\n",
            "Error in batch 0: batch dimension mismatch between logits and logit_lengths\n",
            "Batch contents:\n",
            "{'audio': torch.Size([2, 301, 80]), 'text': torch.Size([2, 200]), 'audio_lengths': torch.Size([2])}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "batch dimension mismatch between logits and logit_lengths",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3a35273a7662>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-80266c03d63b>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             loss = compute_rnnt_loss(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-80266c03d63b>\u001b[0m in \u001b[0;36mcompute_rnnt_loss\u001b[0;34m(logits, targets, input_lengths, target_lengths, blank)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34mf\"input_lengths {input_lengths.shape}, target_lengths {target_lengths.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     return torchaudio.functional.rnnt_loss(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mrnnt_loss\u001b[0;34m(logits, targets, logit_lengths, target_lengths, blank, clamp, reduction, fused_log_softmax)\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0mblank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m     costs, _ = torch.ops.torchaudio.rnnt_loss(\n\u001b[0m\u001b[1;32m   1806\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: batch dimension mismatch between logits and logit_lengths"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Encoder(torch.nn.Module):\n",
        "#     def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "#         super().__init__()\n",
        "#         self.lstm = torch.nn.LSTM(\n",
        "#             input_size=input_dim,\n",
        "#             hidden_size=hidden_dim,\n",
        "#             num_layers=num_layers,\n",
        "#             bidirectional=True,\n",
        "#             batch_first=True,\n",
        "#         )\n",
        "#         self.linear = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x, _ = self.lstm(x)  # (B, T, 2*H)\n",
        "#         x = self.linear(x)    # (B, T, H)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "IR3gNzgsIoEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Decoder(torch.nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "#         super().__init__()\n",
        "#         self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "#         self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "#     def forward(self, y):\n",
        "#         y = self.embed(y)     # (B, U, E)\n",
        "#         y, _ = self.lstm(y)   # (B, U, H)\n",
        "#         return y"
      ],
      "metadata": {
        "id": "V5Ima62TIqXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class JointNetwork(torch.nn.Module):\n",
        "#     def __init__(self, hidden_dim, vocab_size):\n",
        "#         super().__init__()\n",
        "#         self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, h_enc, h_dec):\n",
        "#         # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "#         h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "#         h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "#         out = torch.tanh(h_enc + h_dec)  # (B, T, U, H)\n",
        "#         return self.linear(out)  # (B, T, U, V)"
      ],
      "metadata": {
        "id": "5K2duKk7IsRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class RNNTransducer(torch.nn.Module):\n",
        "#     def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "#         super().__init__()\n",
        "#         self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "#         self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "#         self.joint = JointNetwork(encoder_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         h_enc = self.encoder(x)  # (B, T, H)\n",
        "#         h_dec = self.decoder(y)  # (B, U, H)\n",
        "#         logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "#         return logits.contiguous()"
      ],
      "metadata": {
        "id": "9ZvXpa3aIuE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Continue from where the notebook left off\n",
        "\n",
        "# # First, let's prepare the dataset class\n",
        "# class AudioTextDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, features, texts, tokenizer, max_text_length=200):\n",
        "#         self.features = features\n",
        "#         self.texts = texts\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_text_length = max_text_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.features)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Get audio features\n",
        "#         audio_feature = self.features[idx]\n",
        "\n",
        "#         # Get and tokenize text\n",
        "#         text = self.texts[idx]\n",
        "#         encoding = self.tokenizer.encode(text)\n",
        "#         text_ids = encoding.ids\n",
        "\n",
        "#         # Add BOS and EOS tokens\n",
        "#         text_ids = [tokenizer.token_to_id(\"<bos>\")] + text_ids + [tokenizer.token_to_id(\"<eos>\")]\n",
        "\n",
        "#         # Pad text sequence\n",
        "#         if len(text_ids) < self.max_text_length:\n",
        "#             pad_amount = self.max_text_length - len(text_ids)\n",
        "#             text_ids = text_ids + [tokenizer.token_to_id(\"<pad>\")] * pad_amount\n",
        "#         else:\n",
        "#             text_ids = text_ids[:self.max_text_length]\n",
        "\n",
        "#         return {\n",
        "#             'audio': audio_feature,\n",
        "#             'text': torch.tensor(text_ids, dtype=torch.long)\n",
        "#         }"
      ],
      "metadata": {
        "id": "3K1AB6iTIvDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create datasets\n",
        "# train_dataset = AudioTextDataset(\n",
        "#     features=features,\n",
        "#     texts=cleaned_texts,\n",
        "#     tokenizer=tokenizer,\n",
        "# )"
      ],
      "metadata": {
        "id": "rMMOxn6d2v_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_cleaned_texts = [clean_text(dev_df.loc[idx, 'text']) for idx in test_df.indices]\n",
        "# test_features = []\n",
        "# audio_directory = 'cv-valid-dev'\n",
        "# audio_files = [test_df.dataset.loc[test_df.indices[i], 'filename'] for i in range(len(test_df))]\n",
        "# for audio in audio_files:\n",
        "#   audio_feature = preprocess_for_rnnt_torchaudio(os.path.join(audio_directory, audio))\n",
        "#   test_features.append(audio_feature)\n",
        "\n",
        "# test_dataset = AudioTextDataset(\n",
        "#     features=test_features,\n",
        "#     texts=test_cleaned_texts,\n",
        "#     tokenizer=tokenizer,\n",
        "# )"
      ],
      "metadata": {
        "id": "yqP8JzSnGA-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HtFUK_q1VQQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def collate_fn(batch):\n",
        "#     # Pad audio features to same length\n",
        "#     audio_features = [item['audio'] for item in batch]\n",
        "#     max_audio_len = max(f.size(0) for f in audio_features)\n",
        "\n",
        "#     padded_audio = []\n",
        "#     audio_lengths = []\n",
        "\n",
        "#     for f in audio_features:\n",
        "#         audio_lengths.append(f.size(0))  # Original length before padding\n",
        "#         pad_amount = max_audio_len - f.size(0)\n",
        "#         if pad_amount > 0:\n",
        "#             f = torch.nn.functional.pad(f, (0, 0, 0, pad_amount), value=0)\n",
        "#         padded_audio.append(f)\n",
        "\n",
        "#     audio_tensor = torch.stack(padded_audio)\n",
        "#     text_tensor = torch.stack([item['text'] for item in batch])\n",
        "\n",
        "#     return {\n",
        "#         'audio': audio_tensor.to(device),\n",
        "#         'text': text_tensor.to(device),\n",
        "#         'audio_lengths': torch.tensor(audio_lengths, dtype=torch.int32, device=device)\n",
        "#     }"
      ],
      "metadata": {
        "id": "83SDmsXp2y_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def rnnt_loss(logits, targets, input_lengths, target_lengths, blank=0):\n",
        "#     # logits: (B, T, U, V)\n",
        "#     # targets: (B, U)\n",
        "#     # input_lengths: (B,) - length of each audio sequence\n",
        "#     # target_lengths: (B,) - length of each text sequence\n",
        "\n",
        "#     # Ensure logits are contiguous\n",
        "#     logits = logits.contiguous()\n",
        "\n",
        "#     # Convert to log-probs\n",
        "#     log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "#     # Permute to (T, B, U, V) for torchaudio\n",
        "#     log_probs = log_probs.permute(1, 0, 2, 3)\n",
        "#     log_probs = log_probs.contiguous()\n",
        "\n",
        "#     # Ensure targets are contiguous and correct dtype\n",
        "#     targets = targets.contiguous().to(torch.int32)\n",
        "\n",
        "#     # Compute RNN-T loss\n",
        "#     loss = torchaudio.functional.rnnt_loss(\n",
        "#         log_probs,\n",
        "#         targets,\n",
        "#         input_lengths,\n",
        "#         target_lengths,\n",
        "#         blank=blank,\n",
        "#         reduction='mean'\n",
        "#     )\n",
        "\n",
        "#     return loss"
      ],
      "metadata": {
        "id": "LXG052Jp219E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_epoch(model, dataloader, optimizer):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch_idx, batch in enumerate(dataloader):\n",
        "#         audio = batch['audio']\n",
        "#         text = batch['text']\n",
        "\n",
        "#         # Fix 1: Use the audio_lengths provided by collate_fn\n",
        "#         input_lengths = batch['audio_lengths']\n",
        "\n",
        "#         # Fix 2: Adjust target_lengths to exclude EOS token\n",
        "#         target_lengths = torch.sum(text != tokenizer.token_to_id(\"<pad>\"), dim=1).to(torch.int32)\n",
        "#         target_lengths = target_lengths - 2  # Subtract 1 for EOS token\n",
        "\n",
        "#         # Forward pass\n",
        "#         optimizer.zero_grad()\n",
        "#         logits = model(audio, text[:, :-1]) # Pass unshifted sequence as target\n",
        "#         logits = logits.permute(1, 0, 2, 3)\n",
        "#         # Compute loss\n",
        "#         loss = rnnt_loss(\n",
        "#             logits,\n",
        "#             text[:, 1:].contiguous(), # targets (shifted by 1 to the right)\n",
        "#             input_lengths,\n",
        "#             target_lengths # Subtract 1 for EOS tokens in targets\n",
        "#         )\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "pKD5Qqfu26ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate(model, dataloader):\n",
        "#     model.eval()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in dataloader:\n",
        "#             audio = batch['audio']\n",
        "#             text = batch['text']\n",
        "\n",
        "#             # Fix 1: Use the audio_lengths provided by collate_fn\n",
        "#             input_lengths = batch['audio_lengths']\n",
        "\n",
        "#             # Fix 2: Adjust target_lengths to exclude EOS token\n",
        "#             target_lengths = torch.tensor([(t != tokenizer.token_to_id(\"<pad>\")).sum() for t in text], dtype=torch.int32, device=device)\n",
        "#             target_lengths = target_lengths - 2  # Subtract 1 for EOS token\n",
        "\n",
        "#             logits = model(audio, text[:, :-1]) # Pass unshifted sequence as target\n",
        "\n",
        "#             # Transpose the logits to match the expected shape (T, B, U, V)\n",
        "#             logits = logits.permute(1, 0, 2, 3)\n",
        "\n",
        "#             loss = rnnt_loss(\n",
        "#                 logits.contiguous(),\n",
        "#                 text[:, 1:].contiguous(),\n",
        "#                 input_lengths,\n",
        "#                 target_lengths # Subtract 1 for EOS in targets\n",
        "#             )\n",
        "\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#     return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "w80gsKPg3BPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     train_dataset,\n",
        "#     batch_size=4,\n",
        "#     shuffle=True,\n",
        "#     collate_fn=collate_fn\n",
        "# )\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=4,\n",
        "#     shuffle=False,\n",
        "#     collate_fn=collate_fn\n",
        "# )"
      ],
      "metadata": {
        "id": "p9sKNUfaq72x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training\n",
        "# num_epochs = 5\n",
        "# for epoch in range(num_epochs):\n",
        "#     train_loss = train_epoch(model, train_loader, optimizer)\n",
        "#     val_loss = evaluate(model, test_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "#     print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "XOUrFrPr3EUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference function\n",
        "def predict(model, audio_feature, tokenizer, max_decode_len=100):\n",
        "    model.eval()\n",
        "\n",
        "    # Add batch dimension\n",
        "    audio_feature = audio_feature.unsqueeze(0).to(device)\n",
        "    input_length = torch.tensor([audio_feature.size(1)], device=device)\n",
        "\n",
        "    # Initialize with BOS token\n",
        "    decoded = [tokenizer.token_to_id(\"<bos>\")]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h_enc = model.encoder(audio_feature)  # (1, T, H)\n",
        "\n",
        "        for _ in range(max_decode_len):\n",
        "            # Get last predicted token\n",
        "            last_token = torch.tensor([decoded[-1]], device=device).unsqueeze(0)\n",
        "\n",
        "            # Decoder step\n",
        "            h_dec = model.decoder(last_token)  # (1, 1, H)\n",
        "\n",
        "            # Joint network\n",
        "            logits = model.joint(h_enc, h_dec)  # (1, T, 1, V)\n",
        "            log_probs = torch.nn.functional.log_softmax(logits.squeeze(2), dim=-1)  # (1, T, V)\n",
        "\n",
        "            # Sum over time (approximate)\n",
        "            scores = log_probs.sum(1)  # (1, V)\n",
        "            next_token = scores.argmax(-1).item()\n",
        "\n",
        "            # Stop if EOS is predicted\n",
        "            if next_token == tokenizer.token_to_id(\"<eos>\"):\n",
        "                break\n",
        "\n",
        "            decoded.append(next_token)\n",
        "\n",
        "    # Convert to text\n",
        "    tokens = tokenizer.decode(decoded[1:])  # Skip BOS\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "VhFykH0z2rKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inference\n",
        "test_idx = 0\n",
        "test_audio = features[test_idx]\n",
        "test_text = cleaned_texts[test_idx]\n",
        "\n",
        "predicted_text = predict(model, test_audio, tokenizer)\n",
        "print(f\"Original: {test_text}\")\n",
        "print(f\"Predicted: {predicted_text}\")"
      ],
      "metadata": {
        "id": "KXJLSqV53G8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}