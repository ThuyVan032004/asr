{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CABTMmlzuHom"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Set permissions"
      ],
      "metadata": {
        "id": "I0g7_nqpfmZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets"
      ],
      "metadata": {
        "id": "jlx2dzbbfPvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "eRWwuyT8Bjbk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_folder = 'checkpoints'\n",
        "destination_folder = 'datasets/checkpoints'  # Create 'checkpoints' inside 'datasets'\n",
        "\n",
        "shutil.move(source_folder, destination_folder)"
      ],
      "metadata": {
        "id": "0PQAOeZRBnhI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mozillaorg/common-voice -p /content/datasets --force"
      ],
      "metadata": {
        "id": "Mg_YAjQ9ttKS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "id": "Gz0vNVrDnO37",
        "outputId": "8df150ac-1b0e-4296-8291-b0b4ec854818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip common-voice.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZm5CsqJgsv5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AeD8v4tcg0Vt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('cv-valid-train.csv')\n",
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "fyjBBRLPg_se"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.info()"
      ],
      "metadata": {
        "id": "CuYIW_7qhPeN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df = dev_df"
      ],
      "metadata": {
        "id": "SPCU8pwmKP0z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio, display\n",
        "# import os\n",
        "\n",
        "# def display_audio(audio_path):\n",
        "#   display(Audio(audio_path))\n",
        "\n",
        "# audio_directory = 'cv-valid-dev/cv-valid-dev'\n",
        "# audio_files = [audio for audio in os.listdir(audio_directory)]\n",
        "\n",
        "# for audio_file in audio_files[2000:2050]:\n",
        "#   audio_path = os.path.join(audio_directory, audio_file)\n",
        "#   display_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "1xLe1TYLnWHC",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df['down_votes'].value_counts()"
      ],
      "metadata": {
        "id": "IUWdPBmgpdNy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df[dev_df['down_votes'] == 0][:10]"
      ],
      "metadata": {
        "id": "32ddDIXgqMh6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " => It seems that the difference between upvotes and downvotes doesn't relate to the quality of audios"
      ],
      "metadata": {
        "id": "BEUFkRQKsBrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing steps**\n",
        "\n"
      ],
      "metadata": {
        "id": "RtbdP2_FvilP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = dev_df.drop(columns=dev_df.columns[dev_df.columns.get_loc('up_votes') : dev_df.columns.get_loc('duration') + 1])"
      ],
      "metadata": {
        "id": "H0BtME5vsS0j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev_df.head()"
      ],
      "metadata": {
        "id": "iSOpjutLyE_N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torchaudio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "L0F0ce2jySqh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "pCJakyA7501a",
        "outputId": "55d38eae-488c-4344-8ea6-ed29cace07a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_directory = 'cv-valid-train'\n",
        "\n",
        "train_size = int(0.8 * len(dev_df))\n",
        "train_df = dev_df.iloc[:train_size]\n",
        "test_df = dev_df.iloc[train_size+1:]\n",
        "\n",
        "train_audio_files = [os.path.join(f) for f in train_df['filename']]\n",
        "train_texts = train_df['text'].tolist()\n",
        "\n",
        "test_audio_files = [os.path.join(f) for f in test_df['filename']]\n",
        "test_texts = test_df['text'].tolist()"
      ],
      "metadata": {
        "id": "mzKLy1FmUBqo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchaudio\n",
        "# import librosa\n",
        "# import os\n",
        "\n",
        "# durations = 0\n",
        "# for audio_file in train_audio_files[:80000]:\n",
        "#   audio_data, sr = librosa.load(os.path.join(audio_directory, audio_file), sr=None)\n",
        "#   # ===== SILENCE REMOVAL =====\n",
        "#   original_duration = len(audio_data) / sr\n",
        "#   audio_data = remove_silence(audio_data, sr)\n",
        "#   new_duration = len(audio_data) / sr\n",
        "#   silence_removed = original_duration - new_duration\n",
        "\n",
        "#   # ===== NOISE REDUCTION =====\n",
        "#   # Only attempt noise reduction if we have enough audio left\n",
        "#   if len(audio_data) > 0.2 * sr:  # At least 200ms remaining\n",
        "#       # Estimate noise from non-silent portions (first 100ms after silence removal)\n",
        "#       noise_samples = int(0.1 * sr)\n",
        "#       noise_profile = audio_data[:min(noise_samples, len(audio_data))]\n",
        "\n",
        "#       reduced_noise = nr.reduce_noise(\n",
        "#           y=audio_data,\n",
        "#           sr=sr,\n",
        "#           y_noise=noise_profile,\n",
        "#           stationary=False,\n",
        "#           prop_decrease=0.8\n",
        "#       )\n",
        "#   else:\n",
        "#       reduced_noise = audio_data  # Skip if too short after silence removal\n",
        "\n",
        "#   # ===== VOLUME NORMALIZATION =====\n",
        "#   rms = np.sqrt(np.mean(reduced_noise**2))\n",
        "#   current_dBFS = 20 * np.log10(max(rms, 1e-10))\n",
        "#   target_dBFS = -20\n",
        "#   gain = min(target_dBFS - current_dBFS, 20)\n",
        "\n",
        "#   if gain > 1:\n",
        "#       amplified_audio = reduced_noise * (10 ** (gain / 20))\n",
        "#       amplified_audio = np.clip(amplified_audio, -1.0, 1.0)\n",
        "#   else:\n",
        "#       amplified_audio = reduced_noise\n",
        "\n",
        "#   # ===== RESAMPLING =====\n",
        "#   target_sr = 16000\n",
        "#   if sr != target_sr:\n",
        "#       resampled_audio = librosa.resample(amplified_audio, orig_sr=sr, target_sr=target_sr)\n",
        "#   else:\n",
        "#       resampled_audio = amplified_audio\n",
        "\n",
        "\n",
        "#   durations += librosa.get_duration(y=resampled_audio, sr=target_sr)\n",
        "\n",
        "# print(\"Durations: \", durations)\n"
      ],
      "metadata": {
        "id": "BezLZ0K4BTaI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7kJ4TUaz0yiE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Handle apostrophes/contractions carefully\n",
        "    text = re.sub(r\"([!\\#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "-IysTsiT0AXS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers\n",
        "from tokenizers import pre_tokenizers"
      ],
      "metadata": {
        "id": "l1u7eUFc30hL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with byte-level BPE (better for names/contractions)\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "# Trainer with larger vocab\n",
        "trainer = trainers.BpeTrainer(\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<blank>\"],\n",
        "    vocab_size=8000,\n",
        "    min_frequency=2,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Better for special characters\n",
        ")\n",
        "\n",
        "# Train on properly cleaned text\n",
        "tokenizer.train_from_iterator([clean_text(t) for t in train_texts], trainer)\n",
        "tokenizer.save(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "3JEz_UWM07ut"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "RQhYjuBSMj0v",
        "outputId": "bf4f226d-4726-43ff-e597-81c42294be71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install noisereduce"
      ],
      "metadata": {
        "id": "17IXsOsYyadF",
        "outputId": "3518076b-b311-4e27-c8c9-f339711ef965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: noisereduce in /usr/local/lib/python3.11/dist-packages (3.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from noisereduce) (1.15.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from noisereduce) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from noisereduce) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from noisereduce) (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from noisereduce) (1.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import noisereduce as nr\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IwEcJmCK_o1j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_silence(audio, sr, top_db=20, frame_length=2048, hop_length=512):\n",
        "    \"\"\"Remove silent segments from audio using librosa's trim function\"\"\"\n",
        "    # Trim leading and trailing silence\n",
        "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db,\n",
        "                                          frame_length=frame_length,\n",
        "                                          hop_length=hop_length)\n",
        "    return trimmed_audio"
      ],
      "metadata": {
        "id": "lT8wUa8Hq4O2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rnnt_torchaudio(file_paths, target_sr=16000, max_duration=5, device='cuda'):\n",
        "    \"\"\"GPU-accelerated batch preprocessing pipeline for RNN-T\n",
        "    Returns:\n",
        "        log_mel: (batch_size, T, 80) tensor of log Mel spectrograms\n",
        "        lengths: (batch_size,) tensor of actual lengths in frames\n",
        "    \"\"\"\n",
        "    # Initialize mel transform once\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "        win_length=400,\n",
        "        n_mels=80,\n",
        "    ).to(device)\n",
        "\n",
        "    mel_specs = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    # Calculate max_len in frames based on max_duration\n",
        "    max_len = int(max_duration * target_sr / 160)  # hop_length is 160\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Load audio with original sample rate\n",
        "        audio_data, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # ===== SILENCE REMOVAL =====\n",
        "        original_duration = len(audio_data) / sr\n",
        "        audio_data = remove_silence(audio_data, sr)\n",
        "        new_duration = len(audio_data) / sr\n",
        "        silence_removed = original_duration - new_duration\n",
        "\n",
        "        # ===== NOISE REDUCTION =====\n",
        "        # Only attempt noise reduction if we have enough audio left\n",
        "        if len(audio_data) > 0.2 * sr:  # At least 200ms remaining\n",
        "            # Estimate noise from non-silent portions (first 100ms after silence removal)\n",
        "            noise_samples = int(0.1 * sr)\n",
        "            noise_profile = audio_data[:min(noise_samples, len(audio_data))]\n",
        "\n",
        "            reduced_noise = nr.reduce_noise(\n",
        "                y=audio_data,\n",
        "                sr=sr,\n",
        "                y_noise=noise_profile,\n",
        "                stationary=False,\n",
        "                prop_decrease=0.8\n",
        "            )\n",
        "        else:\n",
        "            reduced_noise = audio_data  # Skip if too short after silence removal\n",
        "\n",
        "        # ===== VOLUME NORMALIZATION =====\n",
        "        rms = np.sqrt(np.mean(reduced_noise**2))\n",
        "        current_dBFS = 20 * np.log10(max(rms, 1e-10))\n",
        "        target_dBFS = -20\n",
        "        gain = min(target_dBFS - current_dBFS, 20)\n",
        "\n",
        "        if gain > 1:\n",
        "            amplified_audio = reduced_noise * (10 ** (gain / 20))\n",
        "            amplified_audio = np.clip(amplified_audio, -1.0, 1.0)\n",
        "        else:\n",
        "            amplified_audio = reduced_noise\n",
        "\n",
        "        # ===== RESAMPLING =====\n",
        "        target_sr = 16000\n",
        "        if sr != target_sr:\n",
        "            resampled_audio = librosa.resample(amplified_audio, orig_sr=sr, target_sr=target_sr)\n",
        "        else:\n",
        "            resampled_audio = amplified_audio\n",
        "\n",
        "        sf.write(file_path, resampled_audio, target_sr, format='mp3')\n",
        "        audio_tensor, sr = torchaudio.load(file_path)\n",
        "        audio_tensor = audio_tensor.to(device)\n",
        "        # 3. Mel transform\n",
        "        mel_spec = mel_transform(audio_tensor)  # (channels, n_mels, time)\n",
        "        log_mel = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
        "        log_mel = (log_mel - log_mel.min()) / (log_mel.max() - log_mel.min())\n",
        "        # Permute to (channels, time, n_mels)\n",
        "        log_mel = log_mel.permute(0, 2, 1)\n",
        "        # 4. Padding/trimming\n",
        "        if log_mel.size(1) > max_len:\n",
        "            log_mel = log_mel[:, :max_len+1, :]\n",
        "        else:\n",
        "            pad_amount = max_len + 1 - log_mel.size(1)\n",
        "            log_mel = torch.nn.functional.pad(log_mel, (0, 0, 0, pad_amount))\n",
        "\n",
        "        mel_specs.append(log_mel.squeeze(0))  # Remove channel dimension if single channel\n",
        "        audio_lengths.append(log_mel.size(1))\n",
        "\n",
        "    # Stack into batch (batch_size, T, n_mels)\n",
        "    mel_specs = torch.stack(mel_specs)\n",
        "    audio_lengths = torch.tensor(audio_lengths, dtype=torch.int32)\n",
        "\n",
        "    return mel_specs, audio_lengths"
      ],
      "metadata": {
        "id": "MTcU-suGxili"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-DcGDWbuMZ7v"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_mel, audio_lengths = preprocess_for_rnnt_torchaudio(['cv-valid-test/cv-valid-test/sample-000001.mp3'], device=device)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# # Convert the PyTorch tensor to a NumPy array\n",
        "# librosa.display.specshow(\n",
        "#     log_mel.cpu().numpy()[0], # Access the first element if it's a batch of size 1\n",
        "#     # sr=16000,\n",
        "#     x_axis='time',\n",
        "#     y_axis='mel',\n",
        "#     fmax=8000,\n",
        "# )\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.title('Mel Spectrogram')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "o1DX9a-fLcKR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=False,\n",
        "            batch_first=True,\n",
        "            dropout=0.2\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, input_dim)\n",
        "        x, _ = self.lstm(x)  # (B, T, H)\n",
        "        x = self.linear(x)   # (B, T, H)\n",
        "        return x\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            dropout=0.2)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # y: (B, U)\n",
        "        y = self.embed(y)     # (B, U, E)\n",
        "        y, _ = self.lstm(y)   # (B, U, H)\n",
        "        y = self.linear(y)    # (B, U, H)\n",
        "        return y\n",
        "\n",
        "class JointNetwork(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        # Use a more robust approach with a multi-layer network\n",
        "        self.joint = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, h_enc, h_dec):\n",
        "        # h_enc: (B, T, H), h_dec: (B, U, H)\n",
        "\n",
        "        # Get batch size and sequence lengths\n",
        "        batch_size = h_enc.size(0)\n",
        "        T = h_enc.size(1)  # Audio sequence length\n",
        "        U = h_dec.size(1)  # Text sequence length\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        h_enc = h_enc.unsqueeze(2)  # (B, T, 1, H)\n",
        "        h_dec = h_dec.unsqueeze(1)  # (B, 1, U, H)\n",
        "\n",
        "        # Expand to create the full cartesian product for alignment\n",
        "        h_enc = h_enc.expand(-1, -1, U, -1)  # (B, T, U, H)\n",
        "        h_dec = h_dec.expand(-1, T, -1, -1)  # (B, T, U, H)\n",
        "\n",
        "        # Concatenate features\n",
        "        joint = torch.cat([h_enc, h_dec], dim=-1)  # (B, T, U, 2H)\n",
        "\n",
        "        # Apply joint network to get logits\n",
        "        logits = self.joint(joint)  # (B, T, U, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class RNNTransducer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, encoder_dim=256, decoder_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hidden_dim=encoder_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim=decoder_dim)\n",
        "        self.joint = JointNetwork(decoder_dim, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: (B, T, features), y: (B, U)\n",
        "        h_enc = self.encoder(x)  # (B, T, H)\n",
        "        h_dec = self.decoder(y)  # (B, U, H)\n",
        "\n",
        "        # Both should now have the same hidden dimension\n",
        "        logits = self.joint(h_enc, h_dec)  # (B, T, U, V)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "# First, let's prepare the dataset class\n",
        "class AudioTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_files, texts, tokenizer, max_text_length=100,\n",
        "                 target_sr=16000, max_duration=5):\n",
        "        \"\"\"\n",
        "        Dataset class that handles:\n",
        "        - File paths for batch audio processing\n",
        "        - Text sequences with tokenization\n",
        "        - Automatic audio preprocessing in batches\n",
        "        \"\"\"\n",
        "        self.audio_files = [str(f) for f in audio_files]  # Ensure string paths\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.target_sr = target_sr\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns single item with:\n",
        "        - audio_file: Path for batch processing\n",
        "        - text: Raw text string for tokenization in collate\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'audio_file': self.audio_files[idx],\n",
        "            'text': self.texts[idx],\n",
        "            'tokenizer': self.tokenizer,  # Pass tokenizer for collate_fn\n",
        "            'max_text_length': self.max_text_length\n",
        "        }\n",
        "\n",
        "def collate_fn(batch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Custom collate function that:\n",
        "    1. Processes audio files in batch using preprocess_for_rnnt_torchaudio\n",
        "    2. Tokenizes and pads text sequences\n",
        "    3. Handles audio length preservation\n",
        "    \"\"\"\n",
        "    # Extract batch components\n",
        "    audio_files = [item['audio_file'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Batch process audio files\n",
        "    audio_features, audio_lengths = preprocess_for_rnnt_torchaudio(\n",
        "        audio_files,\n",
        "        target_sr=batch[0].get('target_sr', 16000),\n",
        "        max_duration=batch[0].get('max_duration', 5),\n",
        "        device=device\n",
        "    )  # (B, T, 80)\n",
        "\n",
        "    target_lengths = [item['max_text_length'] for item in batch]\n",
        "\n",
        "    # Tokenize and pad texts\n",
        "    text_sequences = []\n",
        "    for item in batch:\n",
        "        encoding = item['tokenizer'].encode(item['text'])\n",
        "        text_ids = encoding.ids\n",
        "\n",
        "        # Pad or truncate\n",
        "        max_len = item['max_text_length']\n",
        "        if len(text_ids) < max_len:\n",
        "            text_ids = text_ids + [0] * (max_len - len(text_ids))\n",
        "        else:\n",
        "            text_ids = text_ids[:max_len]\n",
        "\n",
        "        text_sequences.append(torch.tensor(text_ids, dtype=torch.int32))\n",
        "\n",
        "    target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.int32)\n",
        "    # Stack text sequences\n",
        "    text_tensor = torch.stack(text_sequences)\n",
        "\n",
        "    return {\n",
        "        'audio': audio_features.to(device),  # (B, T, 80)\n",
        "        'text': text_tensor.to(device),     # (B, U_max)\n",
        "        'target_lengths': target_lengths_tensor.to(device),\n",
        "        'audio_lengths': torch.tensor(audio_lengths, device=device) # (B,)\n",
        "    }"
      ],
      "metadata": {
        "id": "KPHUco983mhD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7mBpJTF_efH",
        "outputId": "59e97065-e5cb-4271-9759-7e161f20067f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import jiwer"
      ],
      "metadata": {
        "id": "0OiNHUL6jfAW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision=True):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    # Enable mixed precision for faster training and lower memory usage\n",
        "    scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get batch data\n",
        "        audio = batch['audio']  # (B, T, 80)\n",
        "        text = batch['text']    # (B, U)\n",
        "\n",
        "        # Get lengths\n",
        "        input_lengths = batch['audio_lengths']\n",
        "        target_lengths = batch['target_lengths']\n",
        "\n",
        "        print(\"Input length: \", input_lengths)\n",
        "        print(\"Target length: \", target_lengths)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mixed_precision:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Forward pass - encoder\n",
        "                encoder_output = model.encoder(audio)\n",
        "                concat_text = F.pad(text, pad=(1, 0, 0, 0), value=0)\n",
        "                # Forward pass - decoder and joint\n",
        "                h_dec = model.decoder(concat_text)  # (B, U, H)\n",
        "                logits = model.joint(encoder_output, h_dec)  # (B, T, U, V)\n",
        "\n",
        "                print(\"Logits shape: \", logits.shape)\n",
        "                # For targets, use labels shifted right (exclude BOS)\n",
        "                targets = text.contiguous()  # Remove only BOS for targets\n",
        "                print(\"Targets shape: \", targets.shape)\n",
        "                # Compute loss\n",
        "                loss = torchaudio.functional.rnnt_loss(\n",
        "                    logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                    targets=targets,\n",
        "                    logit_lengths=input_lengths,\n",
        "                    target_lengths=target_lengths,\n",
        "                    blank=0,\n",
        "                    reduction='mean'\n",
        "                )\n",
        "\n",
        "            # Backward pass with scaled gradients\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # Lower clip threshold\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Standard training without mixed precision\n",
        "            encoder_output = model.encoder(audio)\n",
        "            concat_text = F.pad(text, pad=(1, 0, 0, 0), value=0)\n",
        "            # Forward pass - decoder and joint\n",
        "            h_dec = model.decoder(concat_text)  # (B, U, H)\n",
        "            logits = model.joint(encoder_output, h_dec)  # (B, T, U, V)\n",
        "\n",
        "            targets = text.contiguous()\n",
        "\n",
        "            loss = torchaudio.functional.rnnt_loss(\n",
        "                logits=torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "                targets=targets,\n",
        "                logit_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=0,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update learning rate with scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    return total_loss / steps"
      ],
      "metadata": {
        "id": "AbKbvmzwtkc3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device, tokenizer):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set using Character Error Rate (CER)\n",
        "\n",
        "    Args:\n",
        "        model: RNNTransducer model\n",
        "        dataloader: DataLoader containing test data\n",
        "        device: Device to run evaluation on\n",
        "        tokenizer: Tokenizer for decoding predictions\n",
        "\n",
        "    Returns:\n",
        "        float: Average CER across all samples\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_cer = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Get batch data\n",
        "            audio = batch['audio']  # (B, T, 80)\n",
        "            text = batch['text']    # (B, U)\n",
        "\n",
        "            # Get lengths\n",
        "            input_lengths = batch['audio_lengths']\n",
        "            target_lengths = batch['target_lengths']\n",
        "\n",
        "            # Forward pass through encoder\n",
        "            encoder_output = model.encoder(audio)  # (B, T, H)\n",
        "\n",
        "            # Initialize decoder input with zero token\n",
        "            zero_token = torch.zeros(1, 1, dtype=torch.long, device=device)\n",
        "\n",
        "            def decode(enc_state, length):\n",
        "                token_list = []\n",
        "\n",
        "                # Initial decoder state\n",
        "                dec_state, hidden = model.decoder(zero_token)\n",
        "\n",
        "                for t in range(length):\n",
        "                    # Get current encoder state\n",
        "                    enc_t = enc_state[t].view(-1)  # (H,)\n",
        "                    dec_t = dec_state.view(-1)     # (H,)\n",
        "\n",
        "                    # Get joint network output\n",
        "                    logits = model.joint(enc_t, dec_t)\n",
        "                    probs = F.softmax(logits, dim=0)\n",
        "                    pred = torch.argmax(probs, dim=0)\n",
        "                    pred = int(pred.item())\n",
        "\n",
        "                    # If not blank token (0), add to sequence and update decoder\n",
        "                    if pred != 0:\n",
        "                        token_list.append(pred)\n",
        "                        token = torch.LongTensor([[pred]]).to(device)\n",
        "                        dec_state, hidden = model.decoder(token, hidden=hidden)\n",
        "\n",
        "                return token_list\n",
        "\n",
        "            # Process each sample in batch\n",
        "            results = []\n",
        "            for i in range(audio.size(0)):\n",
        "                decoded_seq = decode(encoder_output[i], input_lengths[i])\n",
        "                results.append(decoded_seq)\n",
        "\n",
        "            # Calculate CER for each prediction\n",
        "            for i, pred_tokens in enumerate(results):\n",
        "                # Convert predictions to text\n",
        "                pred_text = tokenizer.decode(pred_tokens)\n",
        "                true_text = tokenizer.decode(text[i])\n",
        "\n",
        "                # Calculate CER\n",
        "                cer = jiwer.character_error_rate(true_text, pred_text)\n",
        "                total_cer += cer\n",
        "                num_samples += 1\n",
        "\n",
        "    return total_cer / num_samples"
      ],
      "metadata": {
        "id": "JeVBsUJCnsp5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"cv_tokenizer.json\")"
      ],
      "metadata": {
        "id": "IIa9x7nMhF5c"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in train_audio_files],\n",
        "    texts=train_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "PeDlBCC5Er9j"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = AudioTextDataset(\n",
        "    audio_files=[os.path.join(audio_directory, f) for f in test_audio_files],\n",
        "    texts=test_texts,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HzQQoA7NLvVw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: collate_fn(batch, device=device)\n",
        ")"
      ],
      "metadata": {
        "id": "Lhzo3JhgKWxZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_path = 'checkpoints/rnnt_epoch_94.pt'\n",
        "# checkpoint = torch.load(checkpoint_path)"
      ],
      "metadata": {
        "id": "z0KRKbxDCIdV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union"
      ],
      "metadata": {
        "id": "szj-GdzWKVFm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = RNNTransducer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-3,\n",
        "    weight_decay=1e-6,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "save_dir = 'checkpoints'\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "    # Train for one epoch\n",
        "    train_loss = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=None,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        mixed_precision=True\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss = evaluate(\n",
        "        model=model,\n",
        "        dataloader=test_loader,\n",
        "        device=device,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(save_dir, f\"rnnt_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'test_loss': test_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "ui7LYHodL020",
        "collapsed": true,
        "outputId": "c051d071-8394-4252-fb3b-94f8fcee2d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n",
            "Input length:  tensor([501], device='cuda:0', dtype=torch.int32)\n",
            "Target length:  tensor([100], device='cuda:0', dtype=torch.int32)\n",
            "Logits shape:  torch.Size([1, 501, 101, 8000])\n",
            "Targets shape:  torch.Size([1, 100])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-878ff2416677>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-c9c43c91e12e>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, device, tokenizer, mixed_precision)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Backward pass with scaled gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Lower clip threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r checkpoints.zip checkpoints"
      ],
      "metadata": {
        "id": "Qq77BA25Oiko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoints.zip')"
      ],
      "metadata": {
        "id": "i7MLURLXOmJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}